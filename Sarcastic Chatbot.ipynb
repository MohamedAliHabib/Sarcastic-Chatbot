{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sarcastic Chatbot - LATEST VERSION.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YopInYiPqY8C",
        "colab_type": "code",
        "outputId": "1583ac5d-e3b2-4ada-cf30-8076e4a61857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# download Tensorflow-GPU\n",
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/93/c7bca39b23aae45cd2e85ad3871c81eccc63b9c5276e926511e2e5b0879d/tensorflow_gpu-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 43.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.28.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu) (46.1.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.21.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=cba20ca3fc0a71b78a0075c5a8df54b76fcd4ef31d79d586b0ac313d805b4eb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc2 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc2 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 2.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc2 has requirement tensorflow-estimator<2.3.0,>=2.2.0rc0, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_80WcvqBqrSw",
        "colab_type": "code",
        "outputId": "faad6786-dcd9-4509-cc45-1e4129cae310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uWL0dvoqvHn",
        "colab_type": "text"
      },
      "source": [
        "### Confirm Tensorflow can see the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enhcdNtnqvie",
        "colab_type": "code",
        "outputId": "ca6d52be-9b84-4218-9304-d3b820e23f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0kq0-22olRt",
        "colab_type": "text"
      },
      "source": [
        "For **GPU** information:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmV3P9-oq31e",
        "colab_type": "code",
        "outputId": "c93b6494-9a69-4149-9a31-3048e1d818e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 11998088547608358721, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 17844710373787970104\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 13561090594327992607\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15956161332\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 11176049772546205856\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDxN00toa8v",
        "colab_type": "text"
      },
      "source": [
        "For **CPU** and **RAM** information:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iblH6shwq-5u",
        "colab_type": "code",
        "outputId": "f9c3dbed-815c-4ff8-fa46-02a62096c1e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cat /proc/cpuinfo\n",
        "!cat /proc/meminfo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2300.000\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\n",
            "bogomips\t: 4600.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2300.000\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\n",
            "bogomips\t: 4600.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "MemTotal:       13333556 kB\n",
            "MemFree:         4344484 kB\n",
            "MemAvailable:   12158716 kB\n",
            "Buffers:           97212 kB\n",
            "Cached:          7646144 kB\n",
            "SwapCached:            0 kB\n",
            "Active:           905708 kB\n",
            "Inactive:        7476880 kB\n",
            "Active(anon):     544536 kB\n",
            "Inactive(anon):     8516 kB\n",
            "Active(file):     361172 kB\n",
            "Inactive(file):  7468364 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:             10592 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        639384 kB\n",
            "Mapped:           539880 kB\n",
            "Shmem:              9124 kB\n",
            "Slab:             358076 kB\n",
            "SReclaimable:     309924 kB\n",
            "SUnreclaim:        48152 kB\n",
            "KernelStack:        4464 kB\n",
            "PageTables:         7880 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6666776 kB\n",
            "Committed_AS:    3201820 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:           0 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:              920 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:      203964 kB\n",
            "DirectMap2M:     8183808 kB\n",
            "DirectMap1G:     7340032 kB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUDLOxLcrArP",
        "colab_type": "text"
      },
      "source": [
        "# Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLbQa53uMtoF",
        "colab_type": "text"
      },
      "source": [
        "Import modules:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgTPJ4EwrDTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF6oeoEJVNAt",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkAM03HywHSr",
        "colab_type": "text"
      },
      "source": [
        "## Load up the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKD-PCkurOOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path):\n",
        "    \"\"\"\n",
        "    Reads the data from the csv file.\n",
        "    Arguments:\n",
        "        path: a string.\n",
        "    Returns:\n",
        "        dataset: a Pandas Dataframe with two columns, namely: `Question` and `Answer`.\n",
        "        X: a NumPy array representing `Question` column.\n",
        "        Y: a NumPy array representing `Answer` column.\n",
        "    \"\"\"\n",
        "    \n",
        "    # read the csv file into a pandas dataframe\n",
        "    dataset = pd.read_csv(path, usecols=['Question', 'Answer'])\n",
        "    # make sure all cell values are strings; because some of them \n",
        "    # only contain numbers, so they maybe mistaken with other types.\n",
        "    dataset = dataset.applymap(str)\n",
        "    # shuffle the rows of the dataframe and then reset the index\n",
        "    dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "    \n",
        "    X = np.asarray(dataset['Question'])\n",
        "    Y = np.asarray(dataset['Answer'])\n",
        "    \n",
        "    X = np.apply_along_axis(lambda sen: '<start> '+ sen + ' <end>', 0, X)\n",
        "    Y = np.apply_along_axis(lambda sen: '<start> '+ sen + ' <end>', 0, Y)\n",
        "    \n",
        "    return dataset, X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgg1MT_brpLi",
        "colab_type": "code",
        "outputId": "1b0c556e-767e-4af8-882b-d46897eb40a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "files_path = '/content/drive/My Drive/chatbot/'\n",
        "qa_dataframe, X, Y = load_dataset(files_path + 'dataset.csv')\n",
        "qa_dataframe.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what is winrar ' s favorite pickup line ?</td>\n",
              "      <td>i can unzip it for you .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what did the muslim terrorist say ?</td>\n",
              "      <td>who you gota blow to get some virgins around here</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>why did sjws call out medusa ?</td>\n",
              "      <td>she kept objectifying people .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what does hillary and a rape victim have in co...</td>\n",
              "      <td>they ' ve both been f * * * * * by a rapist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>why does it take southerners so long to do the...</td>\n",
              "      <td>because slavery is illegal .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question                                             Answer\n",
              "0          what is winrar ' s favorite pickup line ?                           i can unzip it for you .\n",
              "1                what did the muslim terrorist say ?  who you gota blow to get some virgins around here\n",
              "2                     why did sjws call out medusa ?                     she kept objectifying people .\n",
              "3  what does hillary and a rape victim have in co...        they ' ve both been f * * * * * by a rapist\n",
              "4  why does it take southerners so long to do the...                       because slavery is illegal ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idVh2Q_XrsIb",
        "colab_type": "code",
        "outputId": "b082a7d9-604f-4231-e3a7-557cc529348f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"Number of question-answer pairs in the dataset: {len(qa_dataframe)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of question-answer pairs in the dataset: 175671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1iIjrtyRblz",
        "colab_type": "text"
      },
      "source": [
        "Cache all hyperparameters into this dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AvsrJZh-zXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperparameters = dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s4nejLhNPb_",
        "colab_type": "text"
      },
      "source": [
        "**Choose the size of vocabulary:** <br>\n",
        "*This is a hyperparameter that you can play with.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utMZB8-zPZfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzOV85bHNkFA",
        "colab_type": "text"
      },
      "source": [
        "Now, let's tokenize the data, by converting each sentence (question or answer) to a sequence of integers which represent indices of their embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUHSYXPaGbcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(sentences, vocab_size):\n",
        "  \n",
        "    \"\"\"\n",
        "    Using Tensorflow Tokenizer to turn each sentence into a sequence of integers \n",
        "    (each integer being the index of a token in a dictionary).\n",
        "    Arguments:\n",
        "      X: a list or a NumPy array of strings, where each element is a sentence.\n",
        "    Returns:\n",
        "      tensor: a NumPy ndarray, where each row represents the the sequence of\n",
        "              integers that maps the words of the equivalent sentence in \n",
        "              the `sentences` list to a their indices (for embbedings). shape=(batch_size, )\n",
        "      lang_tokenizer: a Tensorflow Tokenizer which have been fit on `sentences`.\n",
        "    \"\"\" \n",
        "\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, \n",
        "                                                           filters='')\n",
        "    lang_tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "    tensor = lang_tokenizer.texts_to_sequences(sentences)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNpjMDp8Y_hp",
        "colab_type": "code",
        "outputId": "f10fe9f5-2862-4c61-85af-dca9bfcbbf3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# all sentences (Questions & Answers)\n",
        "# we need to fit the tokenizer on all sentences; to create word_index and \n",
        "# index_word mapper dictionaries for the most frequent VOCAB_SIZE= 10,000 words.\n",
        "texts = np.concatenate((X, Y))\n",
        "# tokenize the data\n",
        "tensor, text_tokenizer = tokenize(texts, VOCAB_SIZE)\n",
        "\n",
        "print(f\"texts[0]= {texts[0]}\")\n",
        "print(f\"tensor[0]= {tensor[0]}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "texts[0]= <start> what is winrar ' s favorite pickup line ? <end>\n",
            "tensor[0]= [   1   10   19    4   13  126 2318  422    7    2    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQXWh1u7PAGs",
        "colab_type": "text"
      },
      "source": [
        "As we see, each sentence has been converted to an array with indices that will be used to map words to their embedding vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG5iFxlvL0ck",
        "colab_type": "text"
      },
      "source": [
        "Now, let's extract back `X` (for questions) and `Y` (for answers) arrays from `tensor`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0nqdbehDbY0",
        "colab_type": "code",
        "outputId": "c17857e6-7b6f-4f80-f6d3-f294eb7fc44c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# extract questions and answers back from tensor\n",
        "X, Y = tensor[:len(X)], tensor[len(X):]\n",
        "\n",
        "print(f\"Shape of X (questions): {X.shape}\")\n",
        "print(f\"Shape of Y (answers): {Y.shape}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X (questions): (175671, 32)\n",
            "Shape of Y (answers): (175671, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExvHb1MxP2hF",
        "colab_type": "text"
      },
      "source": [
        "Get the maximum sequence length for both input and target tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAseK821EJcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def max_seq_length(tensor):\n",
        "    \"\"\"\n",
        "    Get maximum sequence length in the corpus. And, make sure that all rows in\n",
        "    `tensor` has the same length.\n",
        "    Arguments:\n",
        "      tensor: a NumPy ndarray of shape (batch_size,) where each row represents\n",
        "      indices mapping to words in equivalent sentences.\n",
        "    Returns:\n",
        "      max_len: an integer representing maximum sequence length.\n",
        "    \"\"\"\n",
        "    batch_size = len(tensor)\n",
        "    lengths = [len(sentence) for sentence in tensor]\n",
        "    max_len = max(lengths)\n",
        "\n",
        "    # check if all rows in `tensor` has the same length (equal to max_len)\n",
        "    assert lengths == [max_len]*batch_size\n",
        "\n",
        "    return max_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gmSrXO4EJff",
        "colab_type": "code",
        "outputId": "8949410d-c77e-4821-da6e-f78e3e51e6cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Get max_seq_length of input and target tensors\n",
        "max_length_inp, max_length_targ = max_seq_length(X), max_seq_length(Y)\n",
        "print(f\"Maximum sequence length for input (questions) tensor: {max_length_inp}\")\n",
        "print(f\"Maximum sequence length for target (answers) tensor: {max_length_targ}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum sequence length for input (questions) tensor: 32\n",
            "Maximum sequence length for target (answers) tensor: 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uOQyFJv9jNf",
        "colab_type": "text"
      },
      "source": [
        "Save the arrays as npy files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li6f3oOhtYkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_arrays_path = os.path.join(files_path, 'data_arrays')\n",
        "# create the folder if it does not exist\n",
        "if not os.path.exists(data_arrays_path):\n",
        "    os.makedirs(data_arrays_path)\n",
        "    \n",
        "np.save(os.path.join(data_arrays_path, 'X.npy'), X)\n",
        "np.save(os.path.join(data_arrays_path, 'Y.npy'), Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCuXnQUyP_Xc",
        "colab_type": "text"
      },
      "source": [
        "## Download GloVe Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "661vy3QQAW0Y",
        "colab_type": "code",
        "outputId": "f5428de6-3237-4145-86a5-e0f4b873d81e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-14 12:47:38--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-14 12:47:38--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-14 12:47:39--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.13MB/s    in 6m 30s  \n",
            "\n",
            "2020-04-14 12:54:09 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEPZi_eQB0H-",
        "colab_type": "code",
        "outputId": "a79d8fc6-838c-4c5f-85ec-dfc66d3707ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqYkJklYB2_z",
        "colab_type": "code",
        "outputId": "431aec8e-a146-4b6b-8c26-57dc865043ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!ls\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t\t   glove.6B.200d.txt  glove.6B.50d.txt\tsample_data\n",
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjTHfT3KIVhl",
        "colab_type": "text"
      },
      "source": [
        "Read GloVe word embeddings into a word to vector dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zv3CVUXB4TW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_glove_vectors(glove_file):\n",
        "    \"\"\"\n",
        "    This function reads GloVe vectors from .txt file and \n",
        "    returns a word to vector dictionary.\n",
        "    Arguments:\n",
        "      glove_file: a string path to GloVe word embeddings file.\n",
        "    Returns:\n",
        "      word_to_vec: a Python dictionary that maps words to their embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    import numpy as np\n",
        "    \n",
        "    # open the file\n",
        "    with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
        "        \n",
        "        words = set()\n",
        "        word_to_vec = {}\n",
        "\n",
        "        # loop over the rows in the file\n",
        "        for line in f:\n",
        "            # read the line, strip it (remove leading and trailing spaces) and split it\n",
        "            line = line.strip().split()\n",
        "            # first item in the list 'line' is the word itself\n",
        "            curr_word = line[0]\n",
        "            # add the word to set of words\n",
        "            words.add(curr_word)\n",
        "            # add the words with its vector representation as a (key, value) pair to the dictionary\n",
        "            word_to_vec[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "\n",
        "    return word_to_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoMuO06nB_Tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_file = \"/content/glove.6B.200d.txt\"\n",
        "word_to_vec = read_glove_vectors(glove_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmAByNMfQ0gx",
        "colab_type": "text"
      },
      "source": [
        "# The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaIKPw-gfjk5",
        "colab_type": "text"
      },
      "source": [
        "**Choosing Hyperparameters:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_ZcnTMBIHgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = X.shape[0]\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE \n",
        "embedding_dim = 200\n",
        "units = 512 \n",
        "vocab_size = VOCAB_SIZE + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmHKNnqyIHmA",
        "colab_type": "code",
        "outputId": "41043a22-8f93-46e6-c9e8-12f44e945eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(f\"Buffer size: {BUFFER_SIZE}, Batch size: {BATCH_SIZE}, Steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"Embedding size: {embedding_dim}, # of units: {units}\")\n",
        "print(f\"Vocab size: {vocab_size}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Buffer size: 175671, Batch size: 128, Steps per epoch: 1372\n",
            "Embedding size: 200, # of units: 512\n",
            "Vocab size: 10001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqz5OWkgLwpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cache these values into the hyperparameters dictionary\n",
        "hyperparameters['buffer_size'] = BUFFER_SIZE\n",
        "hyperparameters['batch_size'] = BATCH_SIZE\n",
        "hyperparameters['steps_per_epoch'] = steps_per_epoch\n",
        "hyperparameters['embedding_dim'] = embedding_dim\n",
        "hyperparameters['units'] = units\n",
        "hyperparameters['vocab_size'] = vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utBZ0IiOKlMt",
        "colab_type": "text"
      },
      "source": [
        "Create the embedding matrix for words in the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KetGXfl0gAwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embedding_matrix(words, word_to_vec, vocab_size, emb_dim):\n",
        "  \"\"\"\n",
        "  Returns an embeddings matrix for the words in the vocabulary.\n",
        "  Arguments:\n",
        "      words: a list of words.\n",
        "      word_to_vec: a dictionary that maps words to their embedding vectors.\n",
        "      vocab_size: an integer which represents the size of the vocabulary.\n",
        "      emb_dim: an integer which represents the dimension of word embeddings.\n",
        "  Returns:\n",
        "      embedding_matrix: a NumPy array with shape of (vocab_size, emb_dim).\n",
        "  \"\"\"\n",
        "\n",
        "  # create embedding matrix\n",
        "  embedding_matrix = np.zeros((vocab_size, emb_dim), dtype=np.float64)\n",
        "\n",
        "  # loop over the words in our vocabulary\n",
        "  for i, word in enumerate(words):\n",
        "    if word in word_to_vec.keys():\n",
        "      # if the current word is in glove vocab, get its glove vector.\n",
        "      embedding_matrix[i, :] = word_to_vec[word]\n",
        "    else:\n",
        "      # if the current word does not exist in the vocabulary, set its vector to zeros.\n",
        "      embedding_matrix[i, :] = np.zeros((emb_dim,), dtype=np.float64)\n",
        "\n",
        "  return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AU2RK0PgtxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# since Tensorflow tokenizer word_index still have all words even when \n",
        "# vocab_size is passed while defining the Tokenizer. \n",
        "# So, we need to grab the first 10,000 words.\n",
        "words = list(text_tokenizer.word_index.keys())[:vocab_size]\n",
        "embedding_matrix = create_embedding_matrix(words, word_to_vec, vocab_size, \n",
        "                                           embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgXmOFb2ib5m",
        "colab_type": "code",
        "outputId": "5d8d0f27-94fa-4281-9a1f-0a423157924a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding matrix shape: (10001, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7vZBgpvQtlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the embedding matrix\n",
        "np.save(os.path.join(data_arrays_path, 'embedding_matrix.npy'), embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy89x2_KLptU",
        "colab_type": "text"
      },
      "source": [
        "Create a tf.data dataset for `X` and `Y` with buffer size equal to `BUFFER_SIZE` and batch size equal to `BATCH_SIZE`, we will use this dataset to generate batches while training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUh2wiMBB_Zo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((X, Y)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7k_FQFsY0m5",
        "colab_type": "text"
      },
      "source": [
        "Example of an input batch that the model will receive while iterating over `dataset` batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXeKGBBBB_eW",
        "colab_type": "code",
        "outputId": "0e0de63d-a467-4ddb-c74d-8b653503eb98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([128, 32]), TensorShape([128, 32]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWoYCyZSTAqW",
        "colab_type": "text"
      },
      "source": [
        "# Model Architecture\n",
        "\n",
        "I'm going to use an encoder-decoder model with Bahdanau's attention, which has been described in detail in this paper:\n",
        "\n",
        "[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025v5)\n",
        "\n",
        "Some properties of the model:\n",
        "\n",
        "* The encoder will be a **bidirectional encoder** with **512 hidden units** for each direction, resulting in **1024 cells**.\n",
        "\n",
        "* Both encoder & decoder will use **LSTM** as the cell.\n",
        "\n",
        "* The decoder will be **unidirectional** with **1024 hidden units**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO0_BbUVTDDI",
        "colab_type": "text"
      },
      "source": [
        "## The Encoder\n",
        "\n",
        "Define the bidirectional encoder architecture which will consist of the following:\n",
        "\n",
        "1. An Embedding layer that will map input sentences to their embeddings. The vocabulary size is equal to `VOCAB_SIZE`=10,000 and the dimension of embedding is 200 (**Note:** although a power of 2 embedding size would be more suitable to speed up training time by increasing cache utilization during data movement, thus reducing bottlenecks, but GloVe embeddings do not come with a power of 2 embedding size).\n",
        "\n",
        "2. A forward & backward (bidirectional) LSTM layer with 512 units for each.\n",
        "\n",
        "The input to the encoder has a shape of *`(batch_size, )`*, which is the input array of sentences (questions).\n",
        "\n",
        "The hidden state arrays of the encoder are all of the shape *`(batch_size, n_units)`*.\n",
        "\n",
        "The output of the encoder has a shape of *`(batch_size, max_sequence_length, n_units x2)`* ;the hidden size of the output is equal to double the size of units since the encoder is bidirectional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIdobIpkB_jq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, enc_units, \n",
        "                 batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
        "                                                   weights=[embedding_matrix], \n",
        "                                                   trainable=True)\n",
        "        self.bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Propogates the input `x` through the bidirectional encoder and returns\n",
        "        the outputs along with hidden states.\n",
        "        Arguments:\n",
        "            x: a tensor with shape (batch_size, max_seq_length)\n",
        "            hidden: a tuple or a list, with four tensors representing hidden\n",
        "                    and memory states for both the forward and backward LSTMs.\n",
        "                    Each of them having a shape of (batch_size, max_seq_length)\n",
        "        Returns:\n",
        "            output: a tensor representing the output of the encoder with a shape\n",
        "                    of (batch size, max_sequence length, units*2)\n",
        "            state_h: a tensor, with forward and backward hidden states of \n",
        "                     the encoder with a shape of (batch size, units*2)\n",
        "            state_c: a tensor, with forward and backward memory cell states of\n",
        "                     the encoder with a shape of (batch size, units*2)\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        output, fstate_h, fstate_c, bstate_h, bstate_c = self.bi_lstm(x, initial_state = hidden)\n",
        "\n",
        "        state_h = tf.keras.layers.Concatenate()([fstate_h, bstate_h])\n",
        "        state_c = tf.keras.layers.Concatenate()([fstate_c, bstate_c])\n",
        "\n",
        "        return output, state_h, state_c\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        # forward_state_h forward hidden state output, backward_state_h backward hidden state output\n",
        "        # forward_state_c forward cell (memory) state output, backward_state_c backward cell (memory) state output\n",
        "        return (tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units)), \n",
        "                tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCePCEFZe7jI",
        "colab_type": "text"
      },
      "source": [
        "Define the encoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE8nRXM1B_mR",
        "colab_type": "code",
        "outputId": "c924d797-c7ae-43e9-9a46-8060b365e967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "encoder = Encoder(vocab_size, embedding_dim, embedding_matrix, units, \n",
        "                  BATCH_SIZE)\n",
        "\n",
        "# usage example\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
        "print (f'Encoder output shape: (batch size, sequence length, units*2) {sample_output.shape}')\n",
        "print (f'Encoder Hidden state shape: (batch size, units*2) {sample_h.shape}')\n",
        "print (f'Encoder Memory state shape: (batch size, units*2) {sample_c.shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units*2) (128, 32, 1024)\n",
            "Encoder Hidden state shape: (batch size, units*2) (128, 1024)\n",
            "Encoder Memory state shape: (batch size, units*2) (128, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8rqTG1fgCp4",
        "colab_type": "text"
      },
      "source": [
        "## Bahdanau Attention Mechanism\n",
        "\n",
        "The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. The below picture and formulas are an example of attention mechanism from [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025v5).\n",
        "\n",
        "<img src='images/attention_mechanism.jpg' > <br>\n",
        "\n",
        "Here are the equations of Bahdanau's attention:\n",
        "\n",
        "<img src='images/attention_equations_1.jpg' > <br>\n",
        "\n",
        "<img src='images/attention_equations_2.jpg' >\n",
        "\n",
        "To learn more about Bahdanau's attention, you can refer to this paper:\n",
        "[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473v7)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h53yt01hB_sY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  \n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units*2)\n",
        "        self.W2 = tf.keras.layers.Dense(units*2)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # query hidden state shape == (batch_size, hidden size)\n",
        "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # values shape == (batch_size, max_len, hidden size)\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        \n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93AH9-8vsPi7",
        "colab_type": "text"
      },
      "source": [
        "Example of using Bahdanau's attention:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38LfW1xUB_vC",
        "colab_type": "code",
        "outputId": "788f0f37-086c-4395-e2ee-76b1706559be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# usage example\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_h, sample_output)\n",
        "\n",
        "print(f\"Attention result shape: (batch size, units*2) {attention_result.shape}\")\n",
        "print(f\"Attention weights shape: (batch_size, sequence_length, 1) {attention_weights.shape}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units*2) (128, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (128, 32, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOn0K4m4TLGT",
        "colab_type": "text"
      },
      "source": [
        "## The Decoder\n",
        "\n",
        "Define the decoder architecture which will consist of the following:\n",
        "\n",
        "1. Bahdanau's attention which is applied to the output of the encoder to get the context which will be passed, along with the embeddings of the decoder input, to the LSTM layer of the decoder.\n",
        "\n",
        "2. An Embedding layer with a vocabulary size of `VOCAB_SIZE`=10,000 words and an embedding dimension of 200. The embedding takes the decoder input which is of the shape *`(batch_size, )`* and outputs a tensor of the shape *`(batch_size, 1, embedding_dim)`*.\n",
        "\n",
        "3. A forward (unidirectional) LSTM layer with 1024 units for each.\n",
        "\n",
        "\n",
        "The shape of the decoder's output is *`(batch_size, vocab_size)`* and the decoder hidden state size is of the shape *`(batch_size, units x2)`*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3k8_WnGB_xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, dec_units, \n",
        "                 batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                                   weights=[embedding_matrix],\n",
        "                                                   trainable=True)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        \"\"\"\n",
        "        Takes decoder input, hidden state and output of the encoder and\n",
        "        returns the decoder's predictions along with decoder hidden state and\n",
        "        attention weights.\n",
        "        Arguments:\n",
        "            x: a tensor with shape (batch_size, 1) which is the decoder input\n",
        "               at some timestep.\n",
        "            hidden: a tensor representing hidden state of the encoder with a\n",
        "                    shape of (batch size, units*2)\n",
        "            enc_output: a tensor representing the encoder's output with a shape\n",
        "                        of (batch size, sequence length, units*2)\n",
        "        Returns:\n",
        "            x: a tensor with shape (batch_size, vocab size) and it's decoder's\n",
        "               output.\n",
        "            state: a tensor that represents the hidden state of the decoder \n",
        "                   and has a shape of (batch_size, units*2)\n",
        "            attention_weights: a tensor that represents attention weights and \n",
        "                               has a shape of (batch_size, sequence_length, 1)\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the LSTM\n",
        "        output, state_h, state_c = self.lstm(x)\n",
        "\n",
        "        state = state_h\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--rJAyfbB9N7",
        "colab_type": "text"
      },
      "source": [
        "Define the decoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gep0WCFaB_2w",
        "colab_type": "code",
        "outputId": "5d07336c-de70-43eb-bdcf-5c0b29c67eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "decoder = Decoder(vocab_size, embedding_dim, embedding_matrix, units*2, \n",
        "                  BATCH_SIZE)\n",
        "\n",
        "# usage example\n",
        "sample_decoder_output, dec_h, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_h, sample_output)\n",
        "\n",
        "print(f'Decoder output shape: (batch_size, vocab size) {sample_decoder_output.shape}')\n",
        "print(f'Decoder hidden state shape: (batch_size, units*2) {dec_h.shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (128, 10001)\n",
            "Decoder hidden state shape: (batch_size, units*2) (128, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ex4nLScgdzo",
        "colab_type": "text"
      },
      "source": [
        "## Defining Optimizer, Loss Function and Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3aO9_P2CCEe",
        "colab_type": "text"
      },
      "source": [
        "Define the loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVpPJMMTB_5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, \n",
        "                                                            reduction='none')\n",
        "\n",
        "def compute_loss(real, pred):\n",
        "    \"\"\"\n",
        "    This function returns the loss for model's predictions on a batch \n",
        "    of data in comparison with the real outputs at a timestep.\n",
        "    Arguments:\n",
        "        real: real output, a Tensorflow tensor with a shape \n",
        "              of: (batch_size, max_seq_length)\n",
        "        pred: model's predictions at a certain timestep, a Tensorflow tensor \n",
        "              with a shape of: (batch_size, max_seq_length)\n",
        "    Returns:\n",
        "        A Tensorflow tensor with the loss.\n",
        "    \"\"\"\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46vyaRv__Txt",
        "colab_type": "text"
      },
      "source": [
        "Define the perplexity metric:<br>\n",
        "\n",
        "**Note:** I was going to incorporate BLEU score as another metric, but to compute BLEU score on a batch of data, it turns out that we need access the data to count the number of n-grams. In order to do that, we can't use Tensorflow's AutoGraph feature `tf.function`. Anyways, I tried it and using `tf.function` was around 2.5 times faster than dropping this feature only to compute BLEU score. In case you want to use BLEU score, you can use nmt's open source implementation which you can find [here](https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py), or you can use NLTK's implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR94kWaIdsju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_perplexity(real, pred):\n",
        "    \"\"\"\n",
        "    This function returns the perplexity for model's predictions on a batch \n",
        "    of data in comparison with the real outputs at a timestep.\n",
        "    Arguments:\n",
        "        real: real output, a Tensorflow tensor with a shape \n",
        "              of: (batch_size, max_seq_length)\n",
        "        pred: model's predictions at a certain timestep, a Tensorflow tensor \n",
        "              with a shape of: (batch_size, max_seq_length)\n",
        "    Returns:\n",
        "        A Tensorflow tensor with the perplexity.\n",
        "    \"\"\"\n",
        "    \n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.cast(tf.pow(math.e, tf.keras.backend.mean(loss_, axis=-1)), \n",
        "                   dtype=tf.keras.backend.floatx())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPvC_Nb9mxsz",
        "colab_type": "text"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ5hzp5Am3eA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = '/content/drive/My Drive/chatbot/training_checkpoints'\n",
        "\n",
        "# create the folder if it does not exist\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THg5P4PSwSuM",
        "colab_type": "text"
      },
      "source": [
        "## TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAReN007uDnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDKHq-u4tmUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_file_name =f\"/content/drive/My Drive/chatbot/logs/metrics_{int(time.time())}\"\n",
        "\n",
        "# create the folder if it does not exist\n",
        "if not os.path.exists(log_file_name):\n",
        "    os.makedirs(log_file_name)\n",
        "    \n",
        "summary_writer = tf.summary.create_file_writer(log_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wfmLBKJWjTx",
        "colab_type": "text"
      },
      "source": [
        "## Define training step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKiyJ63hCNLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    \"\"\"\n",
        "    This function performs a training step for the model on a batch of data.\n",
        "    Arguments:\n",
        "        inp: a tensor, the input to the encoder network which is a batch of \n",
        "              vectors of integers (indices of words) for sentences (questions) \n",
        "              with a shape of (batch_size, encoder_max_seq_len)\n",
        "        targ: a tensor, the real output that the decoder will use to learn \n",
        "              using teacher forcing. It has a shape same as `inp`\n",
        "              which is (batch_size, encoder_max_seq_len)\n",
        "        enc_hidden: a tuple of four tensors, the initial hidden states for the\n",
        "              encoder network, each of the shape (batch_size, n_units*2)\n",
        "    Returns:\n",
        "        batch_loss: loss for the given batch.\n",
        "        batch_acc: accuracy for the given batch.\n",
        "        batch_bleu: bleu score for the given batch.\n",
        "        batch_ppl: perplexity for the given batch.\n",
        "    \"\"\"\n",
        "    \n",
        "    loss = 0\n",
        "    ppl = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Run the input through the encoder and get back the encoder output\n",
        "        # and the hidden states of the encoder.\n",
        "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
        "        \n",
        "        # Set the hidden state of the encoder to be the initial hidden state\n",
        "        # of the decoder.\n",
        "        dec_hidden = enc_h\n",
        "\n",
        "        # Define the decoder input which is basically the '<start>' token,\n",
        "        # for every sentence in the batch.\n",
        "        dec_input = tf.expand_dims([text_tokenizer.word_index['<start>']] \n",
        "                                   * BATCH_SIZE, 1)\n",
        "        \n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        # looping over timesteps\n",
        "        for t in range(1, targ.shape[1]):\n",
        "\n",
        "            # Pass encoder output to the decoder along with the decoder\n",
        "            # input and initial hidden state and get back the predictions\n",
        "            # for this batch at the current timestep with the hidden state \n",
        "            # of the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, \n",
        "                                                 dec_hidden, \n",
        "                                                 enc_output)\n",
        "            # predictions shape: (batch_size, decoder_vocab_size)\n",
        "            # dec_hidden shape: (batch_size, units*2)\n",
        "            # attention weights (3rd output that has been discarded) \n",
        "            # shape: (batch_size, decoder_max_seq_len, 1)\n",
        "\n",
        "            # compute the loss\n",
        "            loss += compute_loss(targ[:, t], predictions)\n",
        "\n",
        "            # compute the perplexity\n",
        "            ppl += compute_perplexity(targ[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    # compute the loss for the batch\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    \n",
        "    # compute the perplexity for the batch\n",
        "    batch_ppl = (ppl / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    # get gradients\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss, batch_ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLdtPOsszDEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Nicely formatted time string\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return f\"{h}:{m}:{round(s,1)}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRDo6UNYELPO",
        "colab_type": "text"
      },
      "source": [
        "## Training the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RezcbmKopZqL",
        "colab_type": "code",
        "outputId": "a476eeb1-860e-4103-8a00-7578b87c4c0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(hyperparameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'buffer_size': 175671, 'batch_size': 128, 'steps_per_epoch': 1372, 'embedding_dim': 200, 'units': 512, 'vocab_size': 10001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT2RYzygUkhQ",
        "colab_type": "text"
      },
      "source": [
        "Let's train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rum3Ec24osaN",
        "colab_type": "code",
        "outputId": "f0ec5c5f-28bc-45d1-c0bf-cab350ce962c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# to cache loss and perlexity over epochs\n",
        "cache = dict({'train_loss': [], 'train_ppl':[]})\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_epoch = time.time()\n",
        "\n",
        "        # Initialize encoder hidden state\n",
        "        enc_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "        total_loss = 0\n",
        "        total_ppl = 0\n",
        "        \n",
        "        # Training the model using the training data\n",
        "        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "            \n",
        "            # Train the model on current batch\n",
        "            (batch_loss, batch_ppl) = train_step(inp, targ, enc_hidden)\n",
        "\n",
        "            total_loss += batch_loss\n",
        "            total_ppl += batch_ppl  \n",
        "\n",
        "            # print loss and perplexity for current batch\n",
        "            if batch % 400 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{EPOCHS} - \"\n",
        "                      f\"batch: {batch}/{steps_per_epoch} - \"\n",
        "                      f\"loss: {batch_loss.numpy()} - ppl: {batch_ppl}\")\n",
        "        \n",
        "        # compute batch loss and perplexity\n",
        "        total_loss = total_loss / steps_per_epoch\n",
        "        total_ppl = total_ppl / steps_per_epoch     \n",
        "\n",
        "        # Log loss and perplexity to TensorBoard for current epoch\n",
        "        with summary_writer.as_default():\n",
        "          tf.summary.scalar('training_loss', total_loss, step=epoch)\n",
        "          tf.summary.scalar('training_perplexity', total_ppl, step=epoch)\n",
        "\n",
        "        # Save (checkpoint) the model every 15 epochs\n",
        "        if ((epoch+1) > 1) and ((epoch+1) % 15 == 0):\n",
        "          checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "          print(f'Saved checkpoint for epoch {epoch+1}/{EPOCHS} to: {checkpoint_prefix}')\n",
        "        \n",
        "\n",
        "        # cache the loss and perplexity for current epoch\n",
        "        cache['train_loss'].append(total_loss)\n",
        "        cache['train_ppl'].append(total_ppl)\n",
        "\n",
        "        # print loss and perplexity for current epoch\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS} - loss: {total_loss} - \"\n",
        "              f\"ppl: {total_ppl}\")\n",
        "        \n",
        "        print(f\"Time taken for epoch ({epoch + 1}): \"\n",
        "              f\"{time.time() - start_epoch} sec\\n\")\n",
        "\n",
        "execution_time = (time.time() - start_time)\n",
        "print(f'Elapsed time: {hms_string(execution_time)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30 - batch: 0/1372 - loss: 2.9858062267303467 - ppl: 1033.2218017578125\n",
            "Epoch 1/30 - batch: 400/1372 - loss: 1.5260246992111206 - ppl: 17.53879165649414\n",
            "Epoch 1/30 - batch: 800/1372 - loss: 1.417420744895935 - ppl: 15.341301918029785\n",
            "Epoch 1/30 - batch: 1200/1372 - loss: 1.0942234992980957 - ppl: 9.542059898376465\n",
            "Epoch 1/30 - loss: 1.4290498495101929 - ppl: 21.27655792236328\n",
            "Time taken for epoch (1): 687.3922619819641 sec\n",
            "\n",
            "Epoch 2/30 - batch: 0/1372 - loss: 1.2983289957046509 - ppl: 10.93855094909668\n",
            "Epoch 2/30 - batch: 400/1372 - loss: 1.2696301937103271 - ppl: 9.363203048706055\n",
            "Epoch 2/30 - batch: 800/1372 - loss: 1.2044914960861206 - ppl: 9.134459495544434\n",
            "Epoch 2/30 - batch: 1200/1372 - loss: 1.174655795097351 - ppl: 8.303559303283691\n",
            "Epoch 2/30 - loss: 1.195639967918396 - ppl: 8.976604461669922\n",
            "Time taken for epoch (2): 640.7851347923279 sec\n",
            "\n",
            "Epoch 3/30 - batch: 0/1372 - loss: 1.1292966604232788 - ppl: 6.629456996917725\n",
            "Epoch 3/30 - batch: 400/1372 - loss: 1.0496177673339844 - ppl: 6.928847789764404\n",
            "Epoch 3/30 - batch: 800/1372 - loss: 1.0852460861206055 - ppl: 7.431587219238281\n",
            "Epoch 3/30 - batch: 1200/1372 - loss: 1.1644442081451416 - ppl: 7.244038105010986\n",
            "Epoch 3/30 - loss: 1.0801887512207031 - ppl: 6.651001453399658\n",
            "Time taken for epoch (3): 640.9541318416595 sec\n",
            "\n",
            "Epoch 4/30 - batch: 0/1372 - loss: 0.8552877306938171 - ppl: 4.2930192947387695\n",
            "Epoch 4/30 - batch: 400/1372 - loss: 1.0643301010131836 - ppl: 6.570652961730957\n",
            "Epoch 4/30 - batch: 800/1372 - loss: 1.0308594703674316 - ppl: 5.800598621368408\n",
            "Epoch 4/30 - batch: 1200/1372 - loss: 1.0160149335861206 - ppl: 5.541085720062256\n",
            "Epoch 4/30 - loss: 0.9861934185028076 - ppl: 5.276992321014404\n",
            "Time taken for epoch (4): 641.240535736084 sec\n",
            "\n",
            "Epoch 5/30 - batch: 0/1372 - loss: 0.899599552154541 - ppl: 4.124838352203369\n",
            "Epoch 5/30 - batch: 400/1372 - loss: 0.9763702154159546 - ppl: 4.59441614151001\n",
            "Epoch 5/30 - batch: 800/1372 - loss: 0.889148473739624 - ppl: 4.454720497131348\n",
            "Epoch 5/30 - batch: 1200/1372 - loss: 0.8271332383155823 - ppl: 4.041018009185791\n",
            "Epoch 5/30 - loss: 0.9013299345970154 - ppl: 4.325679302215576\n",
            "Time taken for epoch (5): 640.4810876846313 sec\n",
            "\n",
            "Epoch 6/30 - batch: 0/1372 - loss: 0.7957607507705688 - ppl: 3.4280786514282227\n",
            "Epoch 6/30 - batch: 400/1372 - loss: 0.7324226498603821 - ppl: 3.1502952575683594\n",
            "Epoch 6/30 - batch: 800/1372 - loss: 0.7426292300224304 - ppl: 3.1436586380004883\n",
            "Epoch 6/30 - batch: 1200/1372 - loss: 0.8539717793464661 - ppl: 3.534435749053955\n",
            "Epoch 6/30 - loss: 0.8219927549362183 - ppl: 3.6208178997039795\n",
            "Time taken for epoch (6): 640.5873613357544 sec\n",
            "\n",
            "Epoch 7/30 - batch: 0/1372 - loss: 0.7246559858322144 - ppl: 2.8596179485321045\n",
            "Epoch 7/30 - batch: 400/1372 - loss: 0.7409834861755371 - ppl: 3.1662769317626953\n",
            "Epoch 7/30 - batch: 800/1372 - loss: 0.6209783554077148 - ppl: 2.69604754447937\n",
            "Epoch 7/30 - batch: 1200/1372 - loss: 0.6900808215141296 - ppl: 2.7636334896087646\n",
            "Epoch 7/30 - loss: 0.7476962208747864 - ppl: 3.0814077854156494\n",
            "Time taken for epoch (7): 641.1696844100952 sec\n",
            "\n",
            "Epoch 8/30 - batch: 0/1372 - loss: 0.6194244027137756 - ppl: 2.613964796066284\n",
            "Epoch 8/30 - batch: 400/1372 - loss: 0.6782832741737366 - ppl: 2.5422842502593994\n",
            "Epoch 8/30 - batch: 800/1372 - loss: 0.7329396605491638 - ppl: 2.705002784729004\n",
            "Epoch 8/30 - batch: 1200/1372 - loss: 0.6752492189407349 - ppl: 2.7324697971343994\n",
            "Epoch 8/30 - loss: 0.6784229278564453 - ppl: 2.667160987854004\n",
            "Time taken for epoch (8): 640.5656886100769 sec\n",
            "\n",
            "Epoch 9/30 - batch: 0/1372 - loss: 0.549686074256897 - ppl: 2.0868167877197266\n",
            "Epoch 9/30 - batch: 400/1372 - loss: 0.546241044998169 - ppl: 2.145817518234253\n",
            "Epoch 9/30 - batch: 800/1372 - loss: 0.5764942765235901 - ppl: 2.534217357635498\n",
            "Epoch 9/30 - batch: 1200/1372 - loss: 0.5869191288948059 - ppl: 2.332906484603882\n",
            "Epoch 9/30 - loss: 0.6142655611038208 - ppl: 2.347360372543335\n",
            "Time taken for epoch (9): 641.285108089447 sec\n",
            "\n",
            "Epoch 10/30 - batch: 0/1372 - loss: 0.5225333571434021 - ppl: 1.9692429304122925\n",
            "Epoch 10/30 - batch: 400/1372 - loss: 0.6147948503494263 - ppl: 2.137007474899292\n",
            "Epoch 10/30 - batch: 800/1372 - loss: 0.4654880166053772 - ppl: 1.8661279678344727\n",
            "Epoch 10/30 - batch: 1200/1372 - loss: 0.5990437269210815 - ppl: 2.2539918422698975\n",
            "Epoch 10/30 - loss: 0.5554952025413513 - ppl: 2.100761651992798\n",
            "Time taken for epoch (10): 641.1806392669678 sec\n",
            "\n",
            "Epoch 11/30 - batch: 0/1372 - loss: 0.45972585678100586 - ppl: 1.7788312435150146\n",
            "Epoch 11/30 - batch: 400/1372 - loss: 0.4576740264892578 - ppl: 1.7813678979873657\n",
            "Epoch 11/30 - batch: 800/1372 - loss: 0.5164276957511902 - ppl: 1.9806733131408691\n",
            "Epoch 11/30 - batch: 1200/1372 - loss: 0.5549111366271973 - ppl: 2.07474422454834\n",
            "Epoch 11/30 - loss: 0.5018131136894226 - ppl: 1.9081937074661255\n",
            "Time taken for epoch (11): 641.5850422382355 sec\n",
            "\n",
            "Epoch 12/30 - batch: 0/1372 - loss: 0.43422967195510864 - ppl: 1.6911354064941406\n",
            "Epoch 12/30 - batch: 400/1372 - loss: 0.3835703134536743 - ppl: 1.6120504140853882\n",
            "Epoch 12/30 - batch: 800/1372 - loss: 0.4305163323879242 - ppl: 1.7397973537445068\n",
            "Epoch 12/30 - batch: 1200/1372 - loss: 0.44675299525260925 - ppl: 1.756184697151184\n",
            "Epoch 12/30 - loss: 0.4535875618457794 - ppl: 1.7578060626983643\n",
            "Time taken for epoch (12): 641.6990737915039 sec\n",
            "\n",
            "Epoch 13/30 - batch: 0/1372 - loss: 0.4323138892650604 - ppl: 1.6825884580612183\n",
            "Epoch 13/30 - batch: 400/1372 - loss: 0.39661794900894165 - ppl: 1.5688154697418213\n",
            "Epoch 13/30 - batch: 800/1372 - loss: 0.4290149211883545 - ppl: 1.6757186651229858\n",
            "Epoch 13/30 - batch: 1200/1372 - loss: 0.4344480335712433 - ppl: 1.6890537738800049\n",
            "Epoch 13/30 - loss: 0.40990960597991943 - ppl: 1.6381175518035889\n",
            "Time taken for epoch (13): 641.717798948288 sec\n",
            "\n",
            "Epoch 14/30 - batch: 0/1372 - loss: 0.29374945163726807 - ppl: 1.4026514291763306\n",
            "Epoch 14/30 - batch: 400/1372 - loss: 0.31788578629493713 - ppl: 1.4459471702575684\n",
            "Epoch 14/30 - batch: 800/1372 - loss: 0.420594185590744 - ppl: 1.6555683612823486\n",
            "Epoch 14/30 - batch: 1200/1372 - loss: 0.3385343551635742 - ppl: 1.488823413848877\n",
            "Epoch 14/30 - loss: 0.37120506167411804 - ppl: 1.5428935289382935\n",
            "Time taken for epoch (14): 641.7925097942352 sec\n",
            "\n",
            "Epoch 15/30 - batch: 0/1372 - loss: 0.2838362753391266 - ppl: 1.3726527690887451\n",
            "Epoch 15/30 - batch: 400/1372 - loss: 0.3545311987400055 - ppl: 1.5007028579711914\n",
            "Epoch 15/30 - batch: 800/1372 - loss: 0.31217944622039795 - ppl: 1.4484789371490479\n",
            "Epoch 15/30 - batch: 1200/1372 - loss: 0.3887549638748169 - ppl: 1.5909639596939087\n",
            "Saved checkpoint for epoch 15/30 to: /content/drive/My Drive/chatbot/training_checkpoints/ckpt\n",
            "Epoch 15/30 - loss: 0.33617034554481506 - ppl: 1.4649146795272827\n",
            "Time taken for epoch (15): 643.1727130413055 sec\n",
            "\n",
            "Epoch 16/30 - batch: 0/1372 - loss: 0.2930121123790741 - ppl: 1.3746172189712524\n",
            "Epoch 16/30 - batch: 400/1372 - loss: 0.3021238148212433 - ppl: 1.39323091506958\n",
            "Epoch 16/30 - batch: 800/1372 - loss: 0.2876918911933899 - ppl: 1.379202961921692\n",
            "Epoch 16/30 - batch: 1200/1372 - loss: 0.31034034490585327 - ppl: 1.4216927289962769\n",
            "Epoch 16/30 - loss: 0.30518224835395813 - ppl: 1.4014071226119995\n",
            "Time taken for epoch (16): 641.5844175815582 sec\n",
            "\n",
            "Epoch 17/30 - batch: 0/1372 - loss: 0.23540368676185608 - ppl: 1.2832087278366089\n",
            "Epoch 17/30 - batch: 400/1372 - loss: 0.2522604167461395 - ppl: 1.3057448863983154\n",
            "Epoch 17/30 - batch: 800/1372 - loss: 0.29196131229400635 - ppl: 1.3686565160751343\n",
            "Epoch 17/30 - batch: 1200/1372 - loss: 0.27053409814834595 - ppl: 1.3369011878967285\n",
            "Epoch 17/30 - loss: 0.27800261974334717 - ppl: 1.349829077720642\n",
            "Time taken for epoch (17): 641.8284442424774 sec\n",
            "\n",
            "Epoch 18/30 - batch: 0/1372 - loss: 0.2012614905834198 - ppl: 1.220023512840271\n",
            "Epoch 18/30 - batch: 400/1372 - loss: 0.24344059824943542 - ppl: 1.2770951986312866\n",
            "Epoch 18/30 - batch: 800/1372 - loss: 0.28154027462005615 - ppl: 1.3553102016448975\n",
            "Epoch 18/30 - batch: 1200/1372 - loss: 0.2882876396179199 - ppl: 1.3642606735229492\n",
            "Epoch 18/30 - loss: 0.25384044647216797 - ppl: 1.306612253189087\n",
            "Time taken for epoch (18): 641.8745703697205 sec\n",
            "\n",
            "Epoch 19/30 - batch: 0/1372 - loss: 0.19398584961891174 - ppl: 1.20667564868927\n",
            "Epoch 19/30 - batch: 400/1372 - loss: 0.22146612405776978 - ppl: 1.2420746088027954\n",
            "Epoch 19/30 - batch: 800/1372 - loss: 0.22588269412517548 - ppl: 1.2625746726989746\n",
            "Epoch 19/30 - batch: 1200/1372 - loss: 0.2777934968471527 - ppl: 1.3335530757904053\n",
            "Epoch 19/30 - loss: 0.2348804920911789 - ppl: 1.2743165493011475\n",
            "Time taken for epoch (19): 642.1351687908173 sec\n",
            "\n",
            "Epoch 20/30 - batch: 0/1372 - loss: 0.22238925099372864 - ppl: 1.2461168766021729\n",
            "Epoch 20/30 - batch: 400/1372 - loss: 0.22276806831359863 - ppl: 1.2462154626846313\n",
            "Epoch 20/30 - batch: 800/1372 - loss: 0.19384565949440002 - ppl: 1.2108033895492554\n",
            "Epoch 20/30 - batch: 1200/1372 - loss: 0.25490716099739075 - ppl: 1.3006242513656616\n",
            "Epoch 20/30 - loss: 0.21347354352474213 - ppl: 1.2397973537445068\n",
            "Time taken for epoch (20): 641.2025260925293 sec\n",
            "\n",
            "Epoch 21/30 - batch: 0/1372 - loss: 0.1762378215789795 - ppl: 1.1833382844924927\n",
            "Epoch 21/30 - batch: 400/1372 - loss: 0.14267399907112122 - ppl: 1.1427550315856934\n",
            "Epoch 21/30 - batch: 800/1372 - loss: 0.22656694054603577 - ppl: 1.2572088241577148\n",
            "Epoch 21/30 - batch: 1200/1372 - loss: 0.22917091846466064 - ppl: 1.2707270383834839\n",
            "Epoch 21/30 - loss: 0.19705457985401154 - ppl: 1.214355707168579\n",
            "Time taken for epoch (21): 641.7496023178101 sec\n",
            "\n",
            "Epoch 22/30 - batch: 0/1372 - loss: 0.13766531646251678 - ppl: 1.1310160160064697\n",
            "Epoch 22/30 - batch: 400/1372 - loss: 0.14283667504787445 - ppl: 1.136513113975525\n",
            "Epoch 22/30 - batch: 800/1372 - loss: 0.22167661786079407 - ppl: 1.2492871284484863\n",
            "Epoch 22/30 - batch: 1200/1372 - loss: 0.1658642590045929 - ppl: 1.1751630306243896\n",
            "Epoch 22/30 - loss: 0.18317130208015442 - ppl: 1.1935983896255493\n",
            "Time taken for epoch (22): 642.0045421123505 sec\n",
            "\n",
            "Epoch 23/30 - batch: 0/1372 - loss: 0.16957174241542816 - ppl: 1.171595573425293\n",
            "Epoch 23/30 - batch: 400/1372 - loss: 0.16930998861789703 - ppl: 1.1803086996078491\n",
            "Epoch 23/30 - batch: 800/1372 - loss: 0.14124707877635956 - ppl: 1.132277250289917\n",
            "Epoch 23/30 - batch: 1200/1372 - loss: 0.19942539930343628 - ppl: 1.2147748470306396\n",
            "Epoch 23/30 - loss: 0.17030635476112366 - ppl: 1.174973964691162\n",
            "Time taken for epoch (23): 641.8566677570343 sec\n",
            "\n",
            "Epoch 24/30 - batch: 0/1372 - loss: 0.14840532839298248 - ppl: 1.1406781673431396\n",
            "Epoch 24/30 - batch: 400/1372 - loss: 0.13661906123161316 - ppl: 1.1282657384872437\n",
            "Epoch 24/30 - batch: 800/1372 - loss: 0.13140833377838135 - ppl: 1.122448205947876\n",
            "Epoch 24/30 - batch: 1200/1372 - loss: 0.17490549385547638 - ppl: 1.1835979223251343\n",
            "Epoch 24/30 - loss: 0.15972021222114563 - ppl: 1.1601687669754028\n",
            "Time taken for epoch (24): 641.9507529735565 sec\n",
            "\n",
            "Epoch 25/30 - batch: 0/1372 - loss: 0.12157076597213745 - ppl: 1.10629141330719\n",
            "Epoch 25/30 - batch: 400/1372 - loss: 0.1346653550863266 - ppl: 1.1262034177780151\n",
            "Epoch 25/30 - batch: 800/1372 - loss: 0.14667028188705444 - ppl: 1.145218849182129\n",
            "Epoch 25/30 - batch: 1200/1372 - loss: 0.14047832787036896 - ppl: 1.1402390003204346\n",
            "Epoch 25/30 - loss: 0.14924225211143494 - ppl: 1.145567774772644\n",
            "Time taken for epoch (25): 640.5612745285034 sec\n",
            "\n",
            "Epoch 26/30 - batch: 0/1372 - loss: 0.09690511226654053 - ppl: 1.0786932706832886\n",
            "Epoch 26/30 - batch: 400/1372 - loss: 0.15368910133838654 - ppl: 1.1468055248260498\n",
            "Epoch 26/30 - batch: 800/1372 - loss: 0.16482576727867126 - ppl: 1.1633970737457275\n",
            "Epoch 26/30 - batch: 1200/1372 - loss: 0.20986537635326385 - ppl: 1.2245447635650635\n",
            "Epoch 26/30 - loss: 0.14025476574897766 - ppl: 1.1334789991378784\n",
            "Time taken for epoch (26): 641.1069269180298 sec\n",
            "\n",
            "Epoch 27/30 - batch: 0/1372 - loss: 0.09921730309724808 - ppl: 1.079007863998413\n",
            "Epoch 27/30 - batch: 400/1372 - loss: 0.12922322750091553 - ppl: 1.1185859441757202\n",
            "Epoch 27/30 - batch: 800/1372 - loss: 0.15912531316280365 - ppl: 1.159533143043518\n",
            "Epoch 27/30 - batch: 1200/1372 - loss: 0.14480407536029816 - ppl: 1.1418848037719727\n",
            "Epoch 27/30 - loss: 0.1330333799123764 - ppl: 1.1238572597503662\n",
            "Time taken for epoch (27): 642.5239238739014 sec\n",
            "\n",
            "Epoch 28/30 - batch: 0/1372 - loss: 0.10932071506977081 - ppl: 1.0923144817352295\n",
            "Epoch 28/30 - batch: 400/1372 - loss: 0.09030595421791077 - ppl: 1.0695918798446655\n",
            "Epoch 28/30 - batch: 800/1372 - loss: 0.14611099660396576 - ppl: 1.1396969556808472\n",
            "Epoch 28/30 - batch: 1200/1372 - loss: 0.1573992520570755 - ppl: 1.1548316478729248\n",
            "Epoch 28/30 - loss: 0.1270100474357605 - ppl: 1.1160508394241333\n",
            "Time taken for epoch (28): 642.9015610218048 sec\n",
            "\n",
            "Epoch 29/30 - batch: 0/1372 - loss: 0.10286973416805267 - ppl: 1.0842430591583252\n",
            "Epoch 29/30 - batch: 400/1372 - loss: 0.10465604811906815 - ppl: 1.0914011001586914\n",
            "Epoch 29/30 - batch: 800/1372 - loss: 0.12773171067237854 - ppl: 1.1154661178588867\n",
            "Epoch 29/30 - batch: 1200/1372 - loss: 0.11888109892606735 - ppl: 1.1059465408325195\n",
            "Epoch 29/30 - loss: 0.1213158443570137 - ppl: 1.1087754964828491\n",
            "Time taken for epoch (29): 638.942985534668 sec\n",
            "\n",
            "Epoch 30/30 - batch: 0/1372 - loss: 0.08038324117660522 - ppl: 1.0567059516906738\n",
            "Epoch 30/30 - batch: 400/1372 - loss: 0.12701699137687683 - ppl: 1.1132019758224487\n",
            "Epoch 30/30 - batch: 800/1372 - loss: 0.11158730834722519 - ppl: 1.0964608192443848\n",
            "Epoch 30/30 - batch: 1200/1372 - loss: 0.12008422613143921 - ppl: 1.1078068017959595\n",
            "Saved checkpoint for epoch 30/30 to: /content/drive/My Drive/chatbot/training_checkpoints/ckpt\n",
            "Epoch 30/30 - loss: 0.11638721078634262 - ppl: 1.102429747581482\n",
            "Time taken for epoch (30): 638.6673140525818 sec\n",
            "\n",
            "Elapsed time: 5:21:26.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzl26GHOVWFZ",
        "colab_type": "text"
      },
      "source": [
        "Plot metrics over time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt0qfO2HfNJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_metrics(cache):\n",
        "    \"\"\"\n",
        "    Plots loss and perplexity over epochs:\n",
        "    Arguemnts:\n",
        "        cache: a dictionary that contains values over epochs\n",
        "               for both loss and perplexity.\n",
        "    \"\"\"\n",
        "    \n",
        "    # since, items of cache['train_loss'] and cache['train_ppl']\n",
        "    # are tensors, so let's extract values from these tensors.\n",
        "    loss = [tensor.numpy() for tensor in cache['train_loss']]\n",
        "    ppl = [tensor.numpy() for tensor in cache['train_ppl']]\n",
        "    \n",
        "    # Plot the loss\n",
        "    plt.figure()\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot the perplexity\n",
        "    plt.figure()\n",
        "    plt.plot(ppl, label='Training Accuracy')\n",
        "    plt.title('Perplexity')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AVQk6Fnf3bP",
        "colab_type": "code",
        "outputId": "4b0e10ce-2621-48f6-e6a7-3f091fc12371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "plot_metrics(cache)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de5xdZX3v8c9377klM5PbzJAEciMICEWBEBE8GFAUgYPirUJaEFvagJdW62lPradHrNZXe9paEUEwKgUvBayCUkUlInIRFCZcA4SES0IScpnc73Pbv/PHXpPsDHsyk7ntmb2+79drv/Zaz3rW2s9ik+9a8+y1nqWIwMzMyl+m1A0wM7Ph4cA3M0sJB76ZWUo48M3MUsKBb2aWEg58M7OUcOCbHYSkWZJCUsUAt/NZSd8arHaZ9Yd8Hb6NVpJWAJOBTmAX8HPgExGxcxA/YxbwMlAZER0jdZtmfeEzfBvt3h0RdcAcYC7w931dUXn+N2Cp4f/ZrSxExBryZ/gnSDpN0kOStkp6UtJZXfUk/UbSlyT9FtgNzE7K/knSI5K2S/qJpEnFPkfSeEnflrRW0hpJ/ygpK6lK0hOS/iKpl5X0W0mfS+Y/L+l7yWbuT963Stop6UxJmyW9oeBzDpO0W1LToP/HstRy4FtZkDQdOB9YC/wM+EdgEvDXwI+6BeelwAKgHliZlH0Y+FNgKtABXNPDR92ULH8dcDJwDvBnEdEGXAJ8QdJxwGeALPClItuYl7xPiIi6iLgPuDVZv8t84J6IaOnL/pv1hQPfRrsfS9oKPAjcB6wG7oqIuyIiFxGLgGbyB4MuN0XEMxHRERHtSdl3I2JJROwC/i/wIUnZwg+SNDnZzqciYldEbAC+AlwMEBFLyB9ofkz+QHNpRHT2cT9uBuZLUjJ/KfDdQ/kPYdabAV15YDYCvDciftU1I+nrwB9KendBnUrg3oL5VUW2U1i2MlmnsVudmUn52v25TKbbujeTP6v/UUQs7+tORMTvJe0GzpK0lvxfEHf2dX2zvnDgW7lZRf5s/c8PUqfYpWnTC6ZnAO3Axm7lq4BWoPEgV9d8Hfgp8C5JZ0TEg338fMgfLC4B1gE/jIi9Pe+C2aFzl46Vm+8B75b0ruSH0xpJZ0ma1st6l0g6XtJY4AvkA/eA7piIWAvcDXxZ0jhJGUlHSToTQNKlwCnAR4C/BG6WVFfks1qAHDC7SNvfRz70v3MoO23WFw58KysRsQq4EPgs+WBdBfwNvf+//l3yP8iuA2rIB3YxHwaqgGeBLcAPgamSZgBXAx+OiJ0R8Z/kfzv4SpE27ibf7fPb5Eqi0wra/hj5vwAe6OMum/WZb7yy1JP0G+B7EVHyO2El3Qi8GhF9vp/ArK/ch282QiR34L6f/OWeZoPOXTpmI4CkLwJLgH+NiJdL3R4rT+7SMTNLCZ/hm5mlxIjsw29sbIxZs2aVuhlmZqPG4sWLN0bEQcdeGpGBP2vWLJqbm0vdDDOzUUPSyt7quEvHzCwlHPhmZinhwDczSwkHvplZSjjwzcxSwoFvZpYSDnwzs5Qom8CPCK65Zzn3LfMjQM3MiimbwJfEN+9/iXuXbih1U8zMRqSyCXyAxvpqNu5sLXUzzMxGpLIK/IbaKjbtbCt1M8zMRqTyCvy6Kjbt8hm+mVkxZRb41Wz0Gb6ZWVFlFfiNddVs2d1GR2eu1E0xMxtxyizwq4iALbvbS90UM7MRp6wCv6G2GsBX6piZFVFWgd9YVwXgK3XMzIooq8BvqMuf4ftKHTOz1+o18CVNl3SvpGclPSPpk0n5JEmLJC1P3if2sP5lSZ3lki4b7B0o1HWG7yt1zMxeqy9n+B3A/4qI44HTgI9LOh74DHBPRBwN3JPMH0DSJOAq4M3AqcBVPR0YBsP4MZVUZOQ+fDOzInoN/IhYGxGPJdM7gOeAI4ALgZuTajcD7y2y+ruARRGxOSK2AIuAcwej4cVIyt985cA3M3uNQ+rDlzQLOBn4PTA5ItYmi9YBk4uscgSwqmB+dVJWbNsLJDVLam5p6f+Ilw211f7R1sysiD4HvqQ64EfApyJie+GyiAggBtKQiFgYEXMjYm5TU1O/t9NQV+UuHTOzIvoU+JIqyYf99yPi9qR4vaSpyfKpQLFxidcA0wvmpyVlQ6bJwyuYmRXVl6t0BHwbeC4i/r1g0Z1A11U3lwE/KbL6L4FzJE1Mfqw9JykbMl0DqOX/6DAzsy59OcP/H8ClwNslPZG8zgf+GXinpOXAO5J5JM2V9C2AiNgMfBF4NHl9ISkbMg111extz7G7rXMoP8bMbNSp6K1CRDwIqIfFZxep3wz8WcH8jcCN/W3goWqs2z+8Qm11r7tnZpYaZXWnLeS7dMA3X5mZdVd2gd+YDKDma/HNzA5UfoFf7zN8M7Niyi7wJ9V2jZjpM3wzs0JlF/jVFVnqayrYtMtn+GZmhcou8CF/pY7vtjUzO1CZBr6HVzAz664sA98DqJmZvVZ5Bn5dlfvwzcy6KcvAb6yrZsvuNjo6c6VuipnZiFGmgV9FBGze7bN8M7MuZRn4+x5m7n58M7N9yjPw99185cA3M+tSloHfWL9/xEwzM8srz8CvdeCbmXXX64Dxkm4ELgA2RMQJSdltwLFJlQnA1og4qci6K4AdQCfQERFzB6ndBzVuTAWVWfnSTDOzAn15QshNwLXAd7oKIuKirmlJXwa2HWT9t0XExv42sD8k0VBbzcYdPsM3M+vSlyde3S9pVrFlyfNuPwS8fXCbNXC++crM7EAD7cN/K7A+Ipb3sDyAuyUtlrTgYBuStEBSs6TmlpaWATYrf2mmh0g2M9tvoIE/H7jlIMvPiIg5wHnAxyXN66liRCyMiLkRMbepqWmAzeoaQM1n+GZmXfod+JIqgPcDt/VUJyLWJO8bgDuAU/v7eYeqa4jkiBiujzQzG9EGcob/DmBpRKwutlBSraT6rmngHGDJAD7vkDTUVtHakWNXW+dwfaSZ2YjWa+BLugV4GDhW0mpJlyeLLqZbd46kwyXdlcxOBh6U9CTwCPCziPjF4DX94PYPr+B+fDMz6NtVOvN7KP9IkbJXgfOT6ZeAEwfYvn5rrOt6mHkrMxtqS9UMM7MRoyzvtIV8Hz7gH27NzBJlG/gNdR5AzcysUPkGfq378M3MCpVt4FdVZBhXU+EB1MzMEmUb+JBci+/hFczMgBQEvrt0zMzyyjrwGzy8gpnZPmUf+D7DNzPLK+/Ar61my+52OjpzpW6KmVnJlXXgdz3bdrN/uDUzK/PAr+0aXsGBb2ZW1oG/bwC1Xe7HNzMr68Bv9PAKZmb7lHXgN+wbQM1n+GZmZR3442oqqMpm3IdvZkaZB74kX4tvZpboyxOvbpS0QdKSgrLPS1oj6YnkdX4P654r6XlJL0j6zGA2vK/yd9s68M3M+nKGfxNwbpHyr0TEScnrru4LJWWB64DzgOOB+ZKOH0hj+6OhtppNvg7fzKz3wI+I+4HN/dj2qcALEfFSRLQBtwIX9mM7A5Lv0nHgm5kNpA//E5KeSrp8JhZZfgSwqmB+dVJWlKQFkpolNbe0tAygWQdqqqtm485WImLQtmlmNhr1N/CvB44CTgLWAl8eaEMiYmFEzI2IuU1NTQPd3D4NdVW0duTY2doxaNs0MxuN+hX4EbE+IjojIgd8k3z3TXdrgOkF89OSsmG1/1GH7tYxs3TrV+BLmlow+z5gSZFqjwJHSzpSUhVwMXBnfz5vILoGUPPwCmaWdhW9VZB0C3AW0ChpNXAVcJakk4AAVgBXJHUPB74VEedHRIekTwC/BLLAjRHxzJDsxUE0JAOotezwGb6ZpVuvgR8R84sUf7uHuq8C5xfM3wW85pLN4dToAdTMzIAyv9MWYFKtB1AzM4MUBH5VRYbxYyo9vIKZpV7ZBz74YeZmZpCSwG+srfZ4OmaWeukI/Poqj6djZqmXisBv8Bm+mVlKAr+uiq2722nvzJW6KWZmJZOKwO+6Fn+Lu3XMLMVSEvj5a/F9pY6ZpVkqAt8PMzczS0nge3gFM7OUBH5DnYdXMDNLReDXV1dQlc3Q4i4dM0uxVAS+JD/b1sxSLxWBD/l+fA+gZmZp1mvgJw8p3yBpSUHZv0pamjzE/A5JE3pYd4WkpyU9Ial5MBt+qBrqPLyCmaVbX87wbwLO7Va2CDghIt4ILAP+7iDrvy0iToqIuf1r4uBoqK1m4w6f4ZtZevUa+BFxP7C5W9ndEdGRzP6O/APKR7TG+io27mojIkrdFDOzkhiMPvw/BX7ew7IA7pa0WNKCg21E0gJJzZKaW1paBqFZB2qsraatI8fO1o7eK5uZlaEBBb6k/wN0AN/vocoZETEHOA/4uKR5PW0rIhZGxNyImNvU1DSQZhXV4OEVzCzl+h34kj4CXAD8cfTQTxIRa5L3DcAdwKn9/byB6hpewVfqmFla9SvwJZ0L/G/gPRGxu4c6tZLqu6aBc4AlxeoOBw+gZmZp15fLMm8BHgaOlbRa0uXAtUA9sCi55PKGpO7hku5KVp0MPCjpSeAR4GcR8Ysh2Ys+8Hg6ZpZ2Fb1ViIj5RYq/3UPdV4Hzk+mXgBMH1LpBNKk2OcPf4TN8M0un1NxpW5nNMGFspc/wzSy1UhP4AA21Hk/HzNIrXYFfV+0RM80stVIV+E0eQM3MUixVge8B1MwszdIV+LXVbN3dTntnrtRNMTMbdukK/OTmq80+yzezFEpV4HfdfLXR/fhmlkIpC3w/zNzM0itVgd/gM3wzS7FUBb7P8M0szVIV+HXVFVRVZNjo4RXMLIVSFfiSaKyt8gBqZpZKqQp8gMb6ag+gZmaplLrA9wBqZpZW6Qt8j6djZinVp8CXdKOkDZKWFJRNkrRI0vLkfWIP616W1Fku6bLBanh/NdRVsXFnGz08htfMrGz19Qz/JuDcbmWfAe6JiKOBe5L5A0iaBFwFvJn8A8yv6unAMFya6qpp68yxo7WjlM0wMxt2fQr8iLgf2Nyt+ELg5mT6ZuC9RVZ9F7AoIjZHxBZgEa89cAyrBl+Lb2YpNZA+/MkRsTaZXkf+oeXdHQGsKphfnZS9hqQFkpolNbe0tAygWQfXUOu7bc0snQblR9vId4gPqFM8IhZGxNyImNvU1DQYzSqqawA1/3BrZmkzkMBfL2kqQPK+oUidNcD0gvlpSVnJdA2vsNFdOmaWMgMJ/DuBrqtuLgN+UqTOL4FzJE1Mfqw9JykrmYm17sM3s3Tq62WZtwAPA8dKWi3pcuCfgXdKWg68I5lH0lxJ3wKIiM3AF4FHk9cXkrKSqcxmmDi20n34ZpY6FX2pFBHze1h0dpG6zcCfFczfCNzYr9YNkYY6D69gZumTujttIT+8gvvwzSxtUhn4jXXV7tIxs9RJaeB7ADUzS5+UBn412/a0s31ve6mbYmY2bFIZ+Gcem7+x69ZHXilxS8zMhk8qA/+N0yZw+uwGbnxwBW0duVI3x8xsWKQy8AGuOHM267bv5c4nXy11U8zMhkVqA//MY5p4/ZR6Ft7/Irmcx8Y3s/KX2sCXxBVnzmbZ+p38ZlmxYYDMzMpLagMf4II3Hs7h42u44b6XSt0UM7Mhl+rAr8xmuPyts3nk5c089sqWUjfHzGxIpTrwAS5+03TGj6lkoc/yzazMpT7wa6sruPS0mfzy2XW81LKz1M0xMxsyqQ98gMveMovKbIZvPvByqZtiZjZkHPhAU301H5gzjR89tpoNO/aWujlmZkOi34Ev6VhJTxS8tkv6VLc6Z0naVlDncwNv8tD487ceSXtnjpsfWlHqppiZDYk+PQClmIh4HjgJQFKW/LNq7yhS9YGIuKC/nzNcZjfV8a7jp/Ddh1fy0bNeR111v//TmJmNSIPVpXM28GJErByk7ZXEFWfOZvveDg+qZmZlabAC/2Lglh6WnS7pSUk/l/QHPW1A0gJJzZKaW1paBqlZh+bkGRM59chJfPvBl2nv9KBqZlZeBhz4kqqA9wD/VWTxY8DMiDgR+Brw4562ExELI2JuRMxtamoaaLP67cozZ7N2217+24OqmVmZGYwz/POAxyJiffcFEbE9InYm03cBlZIaB+Ezh8xZxxzGMZPrWHj/S0R4UDUzKx+DEfjz6aE7R9IUSUqmT00+b9MgfOaQyWTEgnlHsXTdDu5bVpquJTOzoTCgwJdUC7wTuL2g7EpJVyazHwSWSHoSuAa4OEbBafN7TjycKeNq+IaHWzCzMjKgaw8jYhfQ0K3shoLpa4FrB/IZpVBVkeHyM47kS3c9x5OrtnLi9AmlbpKZ2YD5TtseXHzqdOprKlh4v8/yzaw8OPB7UF9TySWnzeTnS9ayYuOuUjfHzGzAHPgH8SdvmUVFJsMXf/qsH4NoZqOeA/8gDhtXw9+d/3ruWbqB6+97sdTNMTMbEAd+Lz7yllm858TD+fLdz/PAcl+maWajlwO/F5L4p/e/gdcdVsdf3vI4a7buKXWTzMz6xYHfB7XVFdxwySm0dwYf+95iWjs6S90kM7ND5sDvo9lNdfzbH57Ik6u38Q///Wypm2Nmdsgc+Ifg3BOmcOWZR/Gfv3+F/2peVermmJkdEgf+Ifrrc47h9NkN/P2Pl7BkzbZSN8fMrM8c+IeoIpvha390MhPHVvHR7y9m2+72UjfJzKxPHPj90FhXzdcvmcO6bXv51G2P+6YsMxsVHPj9NGfGRD53wfHc+3wLX/v1C6VujplZrxz4A3DJaTN5/8lHcPU9y/jN8xtK3Rwzs4Ny4A+AJL70vjdw7OR6PnnrE6zavLvUTTIz65EDf4DGVGX5xqWnkIvgiu8uZuvutlI3ycysqMF4iPkKSU9LekJSc5HlknSNpBckPSVpzkA/c6SZ2VDLNfNP5oUNO/nA9Q+xeovP9M1s5BmsM/y3RcRJETG3yLLzgKOT1wLg+kH6zBHlbccexncuP5WWHa28/+sP8cyrvkbfzEaW4ejSuRD4TuT9DpggaeowfO6wO212Az/86FvIZsRF3/gdDy7fWOommZntMxiBH8DdkhZLWlBk+RFA4TgEq5OyA0haIKlZUnNLy+gdhviYyfXc/rG3MG3iGD7yH49wx+OrS90kMzNgcAL/jIiYQ77r5uOS5vVnIxGxMCLmRsTcpqamQWhW6UwdP4YfXHk6b5o1ib+67Umu/82LRPjmLDMrrQEHfkSsSd43AHcAp3arsgaYXjA/LSkra+NqKrnpT9/Eu088nP/3i6VcdeczdPqOXDMroQEFvqRaSfVd08A5wJJu1e4EPpxcrXMasC0i1g7kc0eL6oosX73oJK6YN5vvPLySj31/MXvbPZa+mZXGQM/wJwMPSnoSeAT4WUT8QtKVkq5M6twFvAS8AHwT+NgAP3NUyWTE351/HJ+74HjufnY9f/yt37Nll6/VN7Php5HYtzx37txobn7NJf2j3l1Pr+VTtz3BtIlj+I+PvImZDbWlbpKZlQlJi3u4NH4f32k7jM5/w1S+d/mb2bSzjXOvfoCbfvuyR9o0s2HjwB9mpx45iZ9/8q28efYkPv/fz3LRwod5qWVnqZtlZingwC+Bwyfku3S+/Icn8vy6HZz31Qf4xn0v+ioeMxtSDvwSkcQHTpnGrz59JvOOaeKffr6U91//EMvW7yh108ysTDnwS+ywcTUsvPQUrpl/Mq9s2sUF1zzItb9eTntnrtRNM7My48AfASTxnhMPZ9Gnz+SdfzCZf7t7Ge+97rcegM3MBpUDfwRprKvmuj+aww2XzGH99lYuvPa3/Osvl7Jjrx+UbmYD58Afgc49YSqL/moe7znxcK6790Xe+i/3ct29L7CztaPUTTOzUcw3Xo1wT6/extW/WsY9SzcwYWwlf/7W2Vz2llnUVVeUumlmNoL05cYrB/4o8eSqrVz9q2Xc+3wLE8dWsmDeUXz49JnUOvjNDAd+WXr8lS1c/avl3LeshUm1VVwxbzaXnj6TsVUOfrM0c+CXscUrt3D1r5bxwPKNNNZVccW8o/ijN8/wGb9ZSjnwU2Dxys18ZdFyHnxhI7VVWS544+F86E3TmTNjApJK3TwzGyYO/BR57JUt3PrIK/z0qbXsbuvkdYfVcdHc6bxvzhE01lWXunlmNsQc+Cm0s7WDnz31Krc9uorHXtlKRUa847jJXPSm6cw7polsxmf9ZuVoSANf0nTgO+QfghLAwoj4arc6ZwE/AV5Oim6PiC/0tm0H/uBYvn4HP2hexe2PrWHTrjamjKvhg6dM44OnTGNWo8fiNysnQx34U4GpEfFY8pjDxcB7I+LZgjpnAX8dERccyrYd+IOrrSPHr5eu57ZHV3HfshZyAUcfVsfZx03m7OMOY86MiT7zNxvl+hL4/b6kI3ku7dpkeoek54AjgGcPuqINu6qKDOeeMJVzT5jK2m17uOvpdfx66Xq+9cBL3HDfi0wcW8lZxx7G2ccdxrxjmhhXU1nqJpvZEBiUPnxJs4D7gRMiYntB+VnAj4DVwKvkz/af6WEbC4AFADNmzDhl5cqVA26XHdz2ve08sGwj9zy3nnuf38CW3e1UZMSbZk3i7OMO4+zjJnOku37MRoVh+dFWUh1wH/CliLi927JxQC4idko6H/hqRBzd2zbdpTP8OnPB469s4VfPbeDXS9ezbH3+KVyHj6/hlFmTmDtzIqfMnMjrp9RTkfUQTGYjzZAHvqRK4KfALyPi3/tQfwUwNyI2HqyeA7/0Xtm0m3uf38AjKzazeMUW1m3fC0BtVZaTZkzglJn5g8DJMyZQ7y4gs5Ib6h9tBdwMbI6IT/VQZwqwPiJC0qnAD4GZ0cuHOvBHlohgzdY9LF65heYVW2heuYXn120nF5ARHDtlHHNmTOC4qeM4bmo9x04Z58HdzIbZUAf+GcADwNNA1+OZPgvMAIiIGyR9Avgo0AHsAT4dEQ/1tm0H/si3Y287j7+yleaVW1i8cjNPrdrGjoLhm6dPGsPrp4zLHwSm1PP6qeOYMWmsrwYyGyK+8cqGTddfAUvX7mDpuu08t24HS9du5+WNu+h6NvuYyizHTKnnqMZaZjbUMqtxbP69YSwTxlaVdgfMRrkhvSzTrJAkpk0cy7SJY3nH8ZP3le9t72T5+p08t277voPB717axO2Przlg/fFjKpnVsP8AMKOhlpkNY5k6vobJ42qo9A/FZgPmwLchVVOZ5Q3TxvOGaeMPKN/b3smqzbtZsWk3KzftYsWmXazctJsnVm3lp0+9uu+vAgAJmuqqmTq+hqnjxzB1Qs2+6cMn1DBl/Bia6qqpqvBBwexgHPhWEjWVWY6eXM/Rk+tfs6ytI8earXt4ZfNu1m7dw9pte1m7Lf/+QstOHljewq62ztesN35MJQ11VTTWVdNUV71vuvGA6SomjK2ivrqCjH9PsJRx4NuIU1WR4cjG2h5v+ooIdrR2sHbrXl7dtod12/bSsqOVjTtb2bSzjZadrTy3bjsbd7SyfW/x5wBnM2L8mEomjK1k4tgqJoypZMLYKiaOzZdNGFvFuDGV1NdUMK6mgnE1ldTX5OfHVmU99LSNSg58G3UkMa6mknFTKjl2ymv/QijU1pFj065WNu5oyx8QdrWxdXcbW3e3syV537qnjbXb9vLc2u1s3dPO7iJ/PRTKZkRddUVyMKikrqaCuur8gaC2qoKx1dlkvoLa6nxZbXWWsVX5OjWVWcZWZRlTlWVsZQU1VRmqshkfRGzIOfCtrFVVZPL9/uPH9Hmdve2dbNvTzo697Wzf28H2Pe3s2NuRvNoPeN+evLfsaGVXWwe7WjvY3drJrraOA36H6E02I8ZU7j8Y1FRmqK7IUl2Roboyf0Corsjuny5cnpTvm67IUFO5f93C7VRm8+tXZjNUVWSozGpfmbu4yp8D36ybmiR4J4+r6fc2IoK97bl9B4FdyUFgT1sne9o7e37vmm7rpK0zR1tHjtaOTnbs7aC1PUdbZ47W9k5aO3LJq5P2zsG5tDqb0b4DQGU2Q0UmP12RVbfp/IGiIrN/WUVSP5vU69pWNpPUy4hsUjcrkc1kyGY48F2QzWbIKl8vk9n/nl+na3v7yzIZ9i0rrJfZV5990xnl62SUL8u/CqYz+6eVlGvfcsriLzAHvtkQkMSYpNtmqJ84lsvFvvBv7cixt+uA0J4v29u+f1l7chBp7wzaO5P5zhztHfvnWztydORydHQG7Z1RMJ2jI5e8J+V72oPOXNCRCzo6c3TmgvZcjs7OoD2XLEvW68gFueR9tOoK/sL3jIQoOEAkBxfRrU7yDpDJgNi/HQkENNRW84MrTx+y9jvwzUa5TGb/wWW06Ar+XOTfO7u/IujsTN5zOTpzHLisoCwX+8tzSZ18Ga8piyC/LIJc5P8Sy+WCzmS6MxcE7K9bML+vfrKsMwKicFnXevn5wnX2LUu2xQHz++vU1wxtJDvwzWzYZTKiyr8ZDDvfqWJmlhIOfDOzlHDgm5mlhAPfzCwlHPhmZinhwDczSwkHvplZSjjwzcxSYkQ+4lBSC7Cyn6s3AhsHsTmlVm77A+W3T+W2P1B++1Ru+wOv3aeZEdF0sBVGZOAPhKTm3p7rOJqU2/5A+e1Tue0PlN8+ldv+QP/2yV06ZmYp4cA3M0uJcgz8haVuwCArt/2B8tunctsfKL99Krf9gX7sU9n14ZuZWXHleIZvZmZFOPDNzFKibAJf0rmSnpf0gqTPlLo9g0HSCklPS3pCUnOp29Mfkm6UtEHSkoKySZIWSVqevE8sZRsPRQ/783lJa5Lv6QlJ55eyjYdC0nRJ90p6VtIzkj6ZlI/m76infRqV35OkGkmPSHoy2Z9/SMqPlPT7JPNuk1TV67bKoQ9fUhZYBrwTWA08CsyPiGdL2rABkrQCmBsRo/aGEUnzgJ3AdyLihKTsX4DNEfHPycF5YkT8bSnb2Vc97M/ngZ0R8W+lbFt/SJoKTI2IxyTVA4uB9wIfYfR+Rz3t04cYhd+T8k9Pr42InZIqgQeBTwKfBm6PiFsl3QA8GRHXH2xb5XKGfyrwQkS8FBFtwK3AhSVukwERcT+wuVvxhcDNyfTN5P8xjgo97M+oFRFrI+KxZHoH8BxwBKP7O+ppn0alyNuZzFYmrwDeDvwwKe/Td1QugX8EsKpgfjWj+AsuEMDdkhZLWlDqxgyiyRGxNtzx3jsAAAOcSURBVJleB0wuZWMGySckPZV0+Yya7o9CkmYBJwO/p0y+o277BKP0e5KUlfQEsAFYBLwIbI2IjqRKnzKvXAK/XJ0REXOA84CPJ90JZSXyfYqjvV/xeuAo4CRgLfDl0jbn0EmqA34EfCoithcuG63fUZF9GrXfU0R0RsRJwDTyPRqv7892yiXw1wDTC+anJWWjWkSsSd43AHeQ/6LLwfqkn7Wrv3VDidszIBGxPvkHmQO+ySj7npJ+4R8B34+I25PiUf0dFdun0f49AUTEVuBe4HRggqSKZFGfMq9cAv9R4OjkV+sq4GLgzhK3aUAk1SY/OCGpFjgHWHLwtUaNO4HLkunLgJ+UsC0D1hWMifcxir6n5AfBbwPPRcS/Fywatd9RT/s0Wr8nSU2SJiTTY8hfnPIc+eD/YFKtT99RWVylA5BcYnU1kAVujIgvlbhJAyJpNvmzeoAK4D9H4z5JugU4i/xQruuBq4AfAz8AZpAfBvtDETEqfgjtYX/OIt9NEMAK4IqC/u8RTdIZwAPA00AuKf4s+T7v0fod9bRP8xmF35OkN5L/UTZL/iT9BxHxhSQjbgUmAY8Dl0RE60G3VS6Bb2ZmB1cuXTpmZtYLB76ZWUo48M3MUsKBb2aWEg58M7OUcOBbakjqLBgp8YnBHFVV0qzCETTNRqKK3quYlY09ye3pZqnkM3xLveS5A/+SPHvgEUmvS8pnSfp1MtjWPZJmJOWTJd2RjE/+pKS3JJvKSvpmMmb53cldkUj6y2Rs9qck3Vqi3TRz4FuqjOnWpXNRwbJtEfEG4Fryd2wDfA24OSLeCHwfuCYpvwa4LyJOBOYAzyTlRwPXRcQfAFuBDyTlnwFOTrZz5VDtnFlvfKetpYaknRFRV6R8BfD2iHgpGXRrXUQ0SNpI/kEa7Un52oholNQCTCu8jT0ZhndRRBydzP8tUBkR/yjpF+QfmvJj4McFY5ubDSuf4ZvlRQ/Th6JwHJNO9v9G9j+B68j/NfBowQiHZsPKgW+Wd1HB+8PJ9EPkR14F+GPyA3IB3AN8FPY9mGJ8TxuVlAGmR8S9wN8C44HX/JVhNhx8pmFpMiZ5alCXX0RE16WZEyU9Rf4sfX5S9hfAf0j6G6AF+JOk/JPAQkmXkz+T/yj5B2oUkwW+lxwUBFyTjGluNuzch2+pVw4PizfrC3fpmJmlhM/wzcxSwmf4ZmYp4cA3M0sJB76ZWUo48M3MUsKBb2aWEv8fc9CBn7TLFTMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui2SaQc_-JUJ",
        "colab_type": "text"
      },
      "source": [
        "# Inference Mode\n",
        "\n",
        "Now, it's time to try the model and ask him a few questins :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb-k2JGH-RBQ",
        "colab_type": "text"
      },
      "source": [
        "Helper function: to clean the text the user has entered."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuHOZIXA-QT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    This method takes a string, applies different text preprocessing \n",
        "    (characters replacement, removal of unwanted characters, removal of extra \n",
        "    whitespaces) operations and returns a string.\n",
        "    Arguments:\n",
        "      text: a string.\n",
        "    Returns:\n",
        "      a cleaned version of text.\n",
        "    \"\"\"\n",
        "    \n",
        "    import re\n",
        "    \n",
        "    text = str(text)\n",
        "    \n",
        "    # REPLACEMENT\n",
        "    text = re.sub('\\\"', '\\'', text)\n",
        "    text = re.sub(\"“\", '\\'', text)\n",
        "    text = re.sub(\"”\", '\\'', text)\n",
        "    text = re.sub('’', '\\'', text)\n",
        "    text = re.sub('\\[','(', text)\n",
        "    text = re.sub('\\]',')', text)\n",
        "    text = re.sub('\\{','(', text)\n",
        "    text = re.sub('\\}',')', text)\n",
        "    text = re.sub(\"([?.!,:;'?!+\\-*/=%$@&()])\", r\" \\1 \", text)\n",
        "\n",
        "    pattern = re.compile('[^a-zA-Z0-9_\\.\\,\\:\\;\\'\\?\\!\\+\\-\\*\\/\\=\\%\\$\\@\\&\\(\\)]')\n",
        "    # remove unwanted characters\n",
        "    text = re.sub(pattern, ' ', text)\n",
        "    \n",
        "    # lower case the characters in the string\n",
        "    text = text.lower()\n",
        "    \n",
        "    # REMOVAL OF EXTRA WHITESPACES\n",
        "    # remove duplicated spaces\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    # remove leading and trailing spaces\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzFvYQ7aW5rY",
        "colab_type": "text"
      },
      "source": [
        "The following fucntion passes a sentence that has been entered by the user to the model and returns its answer along with the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV6G7yHovBsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    \"\"\"\n",
        "    This function takes a sentence (question) and returns the model's output\n",
        "    (answer) to it.\n",
        "    Arguemnts:\n",
        "      sentence: a string.\n",
        "    Returns:\n",
        "      result: a string, representing model's output to the input.\n",
        "      sentence: a string, the input sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    # clean the input sentence (question) to prepare for the encoder\n",
        "    sentence = clean_text(sentence)\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "\n",
        "    # tokenize the input sentence and pad zeros if its length is less than\n",
        "    # maximum sequence length.\n",
        "    inputs = [text_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  \n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    # initilize the hidden state of the encoder\n",
        "    enc_hidden = (tf.zeros((1, units)), \n",
        "                  tf.zeros((1, units)), \n",
        "                  tf.zeros((1, units)), \n",
        "                  tf.zeros((1, units)))\n",
        "    \n",
        "    enc_output, enc_hidden, enc_c = encoder(inputs, enc_hidden)\n",
        "    \n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([text_tokenizer.word_index['<start>']], 0)\n",
        "    \n",
        "    # generate answer, where the maximum length for the answer is equal\n",
        "    # to max_length_targ=32\n",
        "    for t in range(max_length_targ):\n",
        "      predictions, dec_hidden, attention_weights = decoder(dec_input, \n",
        "                                                          dec_hidden, \n",
        "                                                          enc_output)\n",
        "\n",
        "      predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "      result += text_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "      if text_tokenizer.index_word[predicted_id] == '<end>':\n",
        "        return result, sentence\n",
        "\n",
        "      # the predicted ID is fed back into the model\n",
        "      dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1T8YcXl-cRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def answer(sentence):\n",
        "  \"\"\"\n",
        "  This function takes an input sentence by the user and prints the \n",
        "  model's answer along with the user's input sentence.\n",
        "  Arguments:\n",
        "      sentence: a string.\n",
        "  \"\"\"\n",
        "\n",
        "  result, sentence = evaluate(sentence)\n",
        "\n",
        "  print(f'INPUT: {sentence}')\n",
        "  print(f'CHATBOT ANSWER: {result}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VadxQMf_asiN",
        "colab_type": "text"
      },
      "source": [
        "Let's try to talk to the chatbot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7syu8jypaeOx",
        "colab_type": "code",
        "outputId": "faef3a9f-14ad-4559-d013-d00844a4c2e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer('tell me something funny')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> tell me something funny <end>\n",
            "CHATBOT ANSWER: yes - what about the word on . com <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj5DUbyDabvF",
        "colab_type": "code",
        "outputId": "a7365272-eedb-4b6c-bbb1-9d6a0c9ca08b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer('What is it that you want in life?')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> what is it that you want in life ? <end>\n",
            "CHATBOT ANSWER: wife : i ' m a butcher . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuTl4LhXaZ_e",
        "colab_type": "code",
        "outputId": "e93ae621-0615-4adf-b7a2-3c83c7dc3317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"you're so fun\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> you ' re so fun <end>\n",
            "CHATBOT ANSWER: i ' m in your body ! <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoRLRC44aU-I",
        "colab_type": "code",
        "outputId": "f3421a98-4f2e-44c8-ab03-5c19a111e8cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"do you like annoying people?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> do you like annoying people ? <end>\n",
            "CHATBOT ANSWER: because i ' m only good jokes . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwMnOeEzaRzF",
        "colab_type": "code",
        "outputId": "636b3dce-8543-488b-c321-de81c5833f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer('are you my friend?')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> are you my friend ? <end>\n",
            "CHATBOT ANSWER: because i wanna slam you on my back face <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbH0tRdGaOIZ",
        "colab_type": "code",
        "outputId": "e0babc94-831a-41ef-e966-9db5231005d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"what is your favorite movie?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> what is your favorite movie ? <end>\n",
            "CHATBOT ANSWER: me : that is so good . . . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rkYdl5naL9f",
        "colab_type": "code",
        "outputId": "f59ddd2c-77f6-4bcc-862b-00f6358e310b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"how do you sleep at night?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> how do you sleep at night ? <end>\n",
            "CHATBOT ANSWER: dude , the door is . . . how about you later . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqsgrDV_aGin",
        "colab_type": "code",
        "outputId": "9bbc2e12-6519-4994-ed6e-ce1bb0863666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"what will happen if you went inside a black hole?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> what will happen if you went inside a black hole ? <end>\n",
            "CHATBOT ANSWER: i don ' t know either . it must be robin . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnbKBDDkaDxN",
        "colab_type": "code",
        "outputId": "983b466a-daff-4d4d-d581-e6d2ce564e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer('If we count sheep to fall asleep, what do they count?')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> if we count sheep to fall asleep , what do they count ? <end>\n",
            "CHATBOT ANSWER: king : ) <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgUwFIpmZ-e2",
        "colab_type": "code",
        "outputId": "e976febf-04ad-4c8f-9b8c-934b7106d5db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Why do people order double cheese burgers, large fries, and a diet coke?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> why do people order double cheese burgers , large fries , and a diet coke ? <end>\n",
            "CHATBOT ANSWER: . . . because they ' re always standing on . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hMc7t6sZ6mX",
        "colab_type": "code",
        "outputId": "ade7b2f8-38bf-40ad-f54a-b94273083535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"What is always coming, but never arrives?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> what is always coming , but never arrives ? <end>\n",
            "CHATBOT ANSWER: older <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqdo212BazOO",
        "colab_type": "code",
        "outputId": "d0c7a3d5-8ff2-43eb-8619-26eea7867a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"who is the most stupid person you know?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> who is the most stupid person you know ? <end>\n",
            "CHATBOT ANSWER: donald trump <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "58a5fba4-508f-401d-b6f0-5d2268f171f8",
        "id": "h6xcoSskZ4JG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"knock knock\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> knock knock <end>\n",
            "CHATBOT ANSWER: who ' s there ? alzheimers alzheimers who ? knock knock <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCzJwnrZa2NT",
        "colab_type": "code",
        "outputId": "ba1e350a-731e-4cfc-c561-5f75bca7a1ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"tell me a joke\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> tell me a joke <end>\n",
            "CHATBOT ANSWER: what about your and i ' m still working on it . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeLQ2jaUZzMV",
        "colab_type": "code",
        "outputId": "c9ae247a-ff52-460d-8f7c-6007a1c8089b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Do you exercise?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> do you exercise ? <end>\n",
            "CHATBOT ANSWER: don ' t even ! ! ! that ' s there be dragon ! <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FqH-985ZsY_",
        "colab_type": "code",
        "outputId": "51292844-4dfd-4811-d079-d9d0112e4123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"How do you keep a clear mind during hard times?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> how do you keep a clear mind during hard times ? <end>\n",
            "CHATBOT ANSWER: cut the one up the camera . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JxU-IB2Zo5A",
        "colab_type": "code",
        "outputId": "0eaae450-8581-45aa-809f-07e109179f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"who is your best friend?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> who is your best friend ? <end>\n",
            "CHATBOT ANSWER: robert ' ' yesterday . ' <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQjY971jZmUF",
        "colab_type": "code",
        "outputId": "30691774-b9d5-423f-d2e6-ac9841f0495a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"what do you have in mind?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> what do you have in mind ? <end>\n",
            "CHATBOT ANSWER: well done - old me : ) <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbbNp0KTZkCQ",
        "colab_type": "code",
        "outputId": "c9ced59f-75a9-4792-8f66-24bb3730d7fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"who is you favorite singer?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> who is you favorite singer ? <end>\n",
            "CHATBOT ANSWER: student : hammer . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObU1-zcVZgQP",
        "colab_type": "code",
        "outputId": "b70facac-d35c-4e96-f2ea-8e191d87094b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Are you enjoying this conversation?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> are you enjoying this conversation ? <end>\n",
            "CHATBOT ANSWER: because you ' re 10 / 10 <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxbfLe9rZdYm",
        "colab_type": "code",
        "outputId": "62a2d433-2499-46b1-9e9b-3ef2d34d4aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Do you know Google assistant?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> do you know google assistant ? <end>\n",
            "CHATBOT ANSWER: it doesn ' t really know what comes down ! <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adK4FkkDG8SY",
        "colab_type": "code",
        "outputId": "4b3e0b7c-5386-45ea-b7db-40e26a24c128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"what do you like to talk about the most?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> what do you like to talk about the most ? <end>\n",
            "CHATBOT ANSWER: fire . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0Rj_vP-SaWz",
        "colab_type": "code",
        "outputId": "4ab78c5c-f739-4a8e-e65f-ca6fd7a9a477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Why do you like to talk about fire?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> why do you like to talk about fire ? <end>\n",
            "CHATBOT ANSWER: because it looks like that people should be in the heat . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBE8WFMOWw3J",
        "colab_type": "code",
        "outputId": "0173ff36-6b27-4398-ad7f-7cdd08cda5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"What was the best thing before sliced bread?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> what was the best thing before sliced bread ? <end>\n",
            "CHATBOT ANSWER: massive sandwiches <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8bIrDr4W5p1",
        "colab_type": "code",
        "outputId": "f7f16729-cb4d-40f9-fc27-bf3efc08a5e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"What is better than the FOUNTAIN OF YOUTH?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> what is better than the fountain of youth ? <end>\n",
            "CHATBOT ANSWER: the penis . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jm3qEXuW5yW",
        "colab_type": "code",
        "outputId": "d53e9677-a723-4595-e6a8-55c19a47e2be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Are you lazy?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> are you lazy ? <end>\n",
            "CHATBOT ANSWER: or are you a number ! <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqTldlhuW5wZ",
        "colab_type": "code",
        "outputId": "1a1a1c62-04e9-46fd-ddfd-5d9e7855e0f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"How do you know that you'll never quit smoking?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> how do you know that you ' ll never quit smoking ? <end>\n",
            "CHATBOT ANSWER: when you ' ll tell you . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMiG5P69bGno",
        "colab_type": "code",
        "outputId": "af84cc6f-3118-4482-fe04-a4671e7ad957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"When will you quit smoking?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> when will you quit smoking ? <end>\n",
            "CHATBOT ANSWER: a kind of years <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObcXrDuLW5uU",
        "colab_type": "code",
        "outputId": "72e48bd8-9ea3-49d4-c820-87979dd2d73f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Who talks the most?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> who talks the most ? <end>\n",
            "CHATBOT ANSWER: america <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsi433DaX-Jx",
        "colab_type": "code",
        "outputId": "5fef84e5-fcb0-4f22-9d95-1d9bd04b007c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Who lies the most?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> who lies the most ? <end>\n",
            "CHATBOT ANSWER: a : women <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNjqKKXiX-HM",
        "colab_type": "code",
        "outputId": "6752fe48-bc4c-490b-aac4-fd2bf794189b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"What do you know about computers?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> what do you know about computers ? <end>\n",
            "CHATBOT ANSWER: they all have 3 times in their . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCeMKDJAX-Ej",
        "colab_type": "code",
        "outputId": "38e9b1dc-3746-4905-9b84-1158354d002f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Where do like to travel?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> where do like to travel ? <end>\n",
            "CHATBOT ANSWER: the outlet mall . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2_4exWpX-CF",
        "colab_type": "code",
        "outputId": "b3d59808-4775-45dc-974d-28e683aabf08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Which country do you want to visit?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> which country do you want to visit ? <end>\n",
            "CHATBOT ANSWER: 10 . m . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-d41t7cZWXX",
        "colab_type": "code",
        "outputId": "f4749b47-bf09-4767-da37-3865777fce0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Which type of music do you like?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> which type of music do you like ? <end>\n",
            "CHATBOT ANSWER: shower <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0keeyJ0kZWMT",
        "colab_type": "code",
        "outputId": "1cec5e09-0ae7-4e8e-bc2d-6e075d058dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "answer(\"what's your job?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> what ' s your job ? <end>\n",
            "CHATBOT ANSWER: programming . ' what ' s your hobby ' programming . ' what ' s your hobby ' programming . ' what ' s your hobby ' programming . ' what ' s \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEt-SBaldmvf",
        "colab_type": "code",
        "outputId": "f2ffad39-5182-4d6d-cca4-ad492149c144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Are you an artificial intelligence model?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> are you an artificial intelligence model ? <end>\n",
            "CHATBOT ANSWER: because you ' re giving a crack <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h1L16_gZWIw",
        "colab_type": "code",
        "outputId": "58206bac-e2b4-47e1-debd-b6272b31e5f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"How are you doing?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> how are you doing ? <end>\n",
            "CHATBOT ANSWER: and you respond ' i ' m doing ok . ' <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcNoptYreBTF",
        "colab_type": "code",
        "outputId": "172729ed-c2b1-4696-96a7-114ac4ae3f99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"How are you?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> how are you ? <end>\n",
            "CHATBOT ANSWER: bruce wayne : i ' m fine thanks . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXUzRiNEeBYh",
        "colab_type": "code",
        "outputId": "0deb853b-a42f-4d28-dc59-ffaba10693ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"How was your day?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> how was your day ? <end>\n",
            "CHATBOT ANSWER: and he goes : ' ' <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC5Y-4o4eExw",
        "colab_type": "code",
        "outputId": "364c58fb-8766-442e-e006-427ec45b2fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Where do you vacation?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> where do you vacation ? <end>\n",
            "CHATBOT ANSWER: when she went to sleep . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzAGVVLsertg",
        "colab_type": "code",
        "outputId": "698db856-1c7f-41c7-db9a-84477034d5c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Thanks for your time.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> thanks for your time . <end>\n",
            "CHATBOT ANSWER: when i ' ve lost some man in office . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb3tuh4fki8x",
        "colab_type": "code",
        "outputId": "e71ef151-aabb-480a-cdcc-2ecb463b673f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"What is your least favorite food?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> what is your least favorite food ? <end>\n",
            "CHATBOT ANSWER: sin pi ! <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26nN0U8gk8DG",
        "colab_type": "code",
        "outputId": "7f499660-f239-490e-ea5b-d68e95f9ceb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"What is the most hilarious childhood memory you can think of?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> what is the most hilarious childhood memory you can think of ? <end>\n",
            "CHATBOT ANSWER: when you ' re going . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPiZvHRmlCJC",
        "colab_type": "code",
        "outputId": "11f4fbb0-6cd9-4dd1-c35c-9885c5023cdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Are you funny?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> are you funny ? <end>\n",
            "CHATBOT ANSWER: no . ' everybody is poorly executed . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOmKubCvoQ-t",
        "colab_type": "code",
        "outputId": "6f6aef04-6c77-4061-b6cc-5cb6bba58a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"Beethoven or Bach?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> beethoven or bach ? <end>\n",
            "CHATBOT ANSWER: ) <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlhtOtS8pmMM",
        "colab_type": "code",
        "outputId": "953042cc-8403-46f8-9afb-9232910568fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"How do you like your coffee?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> how do you like your coffee ? <end>\n",
            "CHATBOT ANSWER: mine comes never mind , i was going for the pizza ! ' <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEOG_-CMp2NL",
        "colab_type": "code",
        "outputId": "84986c5f-72d0-40e3-ae4b-4c5cbf861107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"What is your favorite sport?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> what is your favorite sport ? <end>\n",
            "CHATBOT ANSWER: sin - . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj9oOKsmp9C8",
        "colab_type": "code",
        "outputId": "2f337f22-15be-427e-bc3c-66737ea1773a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "answer(\"What is your favorite car?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT: <start> what is your favorite car ? <end>\n",
            "CHATBOT ANSWER: more like a car ride <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvGWb0fwjSLA",
        "colab_type": "text"
      },
      "source": [
        "Start a conversation with the chatbot:<br>\n",
        "\n",
        "The following function takes an input sentence from the user and prints the model's answer to it until the user enters `exit()` to finish the conversation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svq4gl13G8a5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chat():\n",
        "    \"\"\"\n",
        "    This function takes input from the user and outputs the chatbot's answer\n",
        "    until the user inputs `exit()`\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Start your conversation with the chatbot!\")\n",
        "    print(\"If you want to end this conversation, enter: exit()\\nHave fun!\")  \n",
        "\n",
        "    while True:\n",
        "      # take user input\n",
        "      user_input = str(input('>>'))\n",
        "      \n",
        "      if user_input == 'exit()':\n",
        "        break\n",
        "\n",
        "      try:\n",
        "        # generate an answer for the user's question.\n",
        "        chatbot_answer, _ = evaluate(user_input)\n",
        "        print(chatbot_answer)\n",
        "      except:\n",
        "        # in case there's word out of chatbot vocabulary.\n",
        "        print(\"Oops! can't help you there! Try different words or restructure your sentence.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_598_iQeG8N5",
        "colab_type": "code",
        "outputId": "3f4c36ac-ad39-4da7-b665-223a68171a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "chat()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start your conversation with the chatbot!\n",
            "If you want to end this conversation, enter: exit()\n",
            "Have fun!\n",
            ">>Are you a man or a woman?\n",
            "well , you can give you a loan <end> \n",
            ">>Are you a chat bot?\n",
            "' cause i can ' t give a car / <end> \n",
            ">>How do you plan for vacations?\n",
            "Oops! can't help you there! Try different words or restructure your sentence.\n",
            ">>How do you plan for holidays?\n",
            "you need to do with it <end> \n",
            ">>do you understand gravity\n",
            "nan <end> \n",
            ">>Are you smart?\n",
            "because you ' re an adult with cheese ? <end> \n",
            ">>stop it\n",
            "http : / / <end> \n",
            ">>what are you doing?\n",
            "me : * climbing out * are you what are you <end> \n",
            ">>Thank you for having this conversation with me\n",
            "why are you on my phone on the wrong foot ? <end> \n",
            ">>okay, that's enough.\n",
            "' . . . ' why , it was too dark . <end> \n",
            ">>Because, it's 10 pm :)\n",
            "there are police , and the u in - the ' is the and the u in - the <end> \n",
            ">>bye\n",
            "what ' s the difference between a lake <end> \n",
            ">>exit()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "breXb3CJdC90",
        "colab_type": "text"
      },
      "source": [
        "As we notice, not all the answers generated by the model are good or valid answers and there's a lot to improve here. For example, there's some answers with repeated blocks of words. Also, some of the answers are not valid responses to the questions and do not relate the topic being proposed in the question. This kind of performance is expected for a deep learning chatbot with this very small dataset but it's okay for a demo project which can be improved.<br>\n",
        "\n",
        "### What can we do to improve performance:\n",
        "\n",
        "**More data**: if we want to have good performance with somewhat impressive responses, we need to train on a lot more data, maybe millions of pairs. However, training on a large scale dataset requires more computational power (maybe a more powerful GPU) and a more sophisticated model with more layers and units, let alone finding and acquiring such dataset.\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQnW_rOwYuq4",
        "colab_type": "text"
      },
      "source": [
        "**DISCLAIMER:** the model has been trained on *175,671* question-answer pairs which have been gathered from different sources like reddit, and it turns out that some of the model's answers are impolite, just something to be aware of."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed42b3bcYP5h",
        "colab_type": "text"
      },
      "source": [
        "# References:\n",
        "\n",
        "* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025v5)\n",
        "\n",
        "* [Neural machine translation with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention) \n",
        "\n",
        "* [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473v7)\n",
        "\n",
        "* [Neural Machine Translation (seq2seq) Tutorial](https://github.com/tensorflow/nmt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT9ymsC6G8Kx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AvYSmDjhcT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}